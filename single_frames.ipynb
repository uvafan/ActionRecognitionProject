{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose(\n",
    "    [transforms.Resize(256),  # 1. Resize smallest side to 256.\n",
    "     transforms.CenterCrop(224), # 2. Crop center square of 224x224 pixels.\n",
    "     transforms.ToTensor(), # 3. Convert to pytorch tensor.\n",
    "     transforms.Normalize(mean = [0.485, 0.456, 0.406],  # normalize.\n",
    "                          std = [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list(cat,d1):\n",
    "    ret = list()\n",
    "    for frame in os.listdir('data/data_first_25/{}/{}'.format(cat,d1)):\n",
    "        img_pil = Image.open('data/data_first_25/{}/{}/{}'.format(cat,d1,frame))\n",
    "        input_img = test_transform(img_pil).unsqueeze(0)\n",
    "        ret.append(input_img)\n",
    "    return ret\n",
    "\n",
    "def createTrainAndValSet(categories,trainPercentage):\n",
    "    category_options = sorted(os.listdir('data/data_first_25'))\n",
    "    category_names = category_options[:categories]\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    i=0\n",
    "    for cat in category_names:\n",
    "        print(cat)\n",
    "        for d1 in os.listdir('data/data_first_25/{}'.format(cat)):\n",
    "            r = random.uniform(0,1)\n",
    "            img_list = get_img_list(cat,d1)\n",
    "#           Adding just a single frame to each frame_list\n",
    "            if int(d1[1:3]) <= trainPercentage * 25:\n",
    "                for img in img_list:\n",
    "                    train_set.append(([img],i))\n",
    "            else:\n",
    "                for img in img_list:\n",
    "                    val_set.append(([img],i))\n",
    "        i+=1\n",
    "    return train_set,val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFrameModel(nn.Module):\n",
    "    def __init__(self, output_size=5):\n",
    "        super(AverageModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.output_size = output_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        result = torch.zeros((1,self.output_size)).cuda()\n",
    "        \n",
    "        x = self.resnet(x_3d[:, 0, :, :, :])  # ResNet\n",
    "        x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_accuracies = []; train_losses = [];\n",
    "val_accuracies = []; val_losses = [];\n",
    "\n",
    "def train_model(model, loss_fn, optimizer, epochs):\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()\n",
    "    batchSize = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        cum_loss = 0\n",
    "\n",
    "        i = 0\n",
    "        model.train()\n",
    "        shuffle(train_set)\n",
    "        for video in train_set:\n",
    "            frame_list, target_cat = video\n",
    "            frame_list = torch.stack(frame_list, dim=0).transpose(0, 1).cuda()\n",
    "            #scores = model(frame_list[:,0,:,:,:])\n",
    "            scores = model(frame_list)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(scores, torch.tensor(np.array([target_cat]),dtype=torch.long).cuda())\n",
    "            max_score, max_label = scores.max(1)\n",
    "            if max_label == target_cat:\n",
    "                correct+=1\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #print(model.fc1.weight)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                    (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
    "            i += 1\n",
    "            \n",
    "        train_accuracies.append(correct / len(train_set))\n",
    "        train_losses.append(cum_loss / (i + 1))   \n",
    "        \n",
    "        i = 0\n",
    "        correct = 0\n",
    "        cum_loss = 0\n",
    "        model.eval()\n",
    "        for video in val_set:\n",
    "            frame_list, target_cat = video\n",
    "            frame_list = torch.stack(frame_list, dim=0).transpose(0, 1).cuda()\n",
    "            #scores = model(frame_list[:,0,:,:,:])\n",
    "            scores = model(frame_list)\n",
    "            \n",
    "            loss = loss_fn(scores, torch.tensor(np.array([target_cat]),dtype=torch.long).cuda())\n",
    "            max_score, max_label = scores.max(1)\n",
    "            if max_label == target_cat:\n",
    "                correct+=1\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            i += 1\n",
    "        print('Validation-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "               (epoch, i + 1, cum_loss / (i + 1), correct / len(val_set)))\n",
    "        \n",
    "        val_accuracies.append(correct / len(val_set))\n",
    "        val_losses.append(cum_loss / (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApplyEyeMakeup\n",
      "ApplyLipstick\n",
      "Archery\n",
      "BabyCrawling\n",
      "BalanceBeam\n"
     ]
    }
   ],
   "source": [
    "categories = 5\n",
    "\n",
    "train_set, val_set = createTrainAndValSet(categories, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00100, Avg-Loss: 3.2142, Accuracy: 0.2100\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 2.8351, Accuracy: 0.1950\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 2.5801, Accuracy: 0.2100\n",
      "Train-epoch 0. Iteration 00400, Avg-Loss: 2.4096, Accuracy: 0.2050\n",
      "Train-epoch 0. Iteration 00500, Avg-Loss: 2.3132, Accuracy: 0.2120\n",
      "Train-epoch 0. Iteration 00600, Avg-Loss: 2.2427, Accuracy: 0.2133\n",
      "Train-epoch 0. Iteration 00700, Avg-Loss: 2.1702, Accuracy: 0.2214\n",
      "Train-epoch 0. Iteration 00800, Avg-Loss: 2.1184, Accuracy: 0.2112\n",
      "Train-epoch 0. Iteration 00900, Avg-Loss: 2.0772, Accuracy: 0.2078\n",
      "Train-epoch 0. Iteration 01000, Avg-Loss: 2.0374, Accuracy: 0.2140\n",
      "Train-epoch 0. Iteration 01100, Avg-Loss: 2.0067, Accuracy: 0.2091\n",
      "Train-epoch 0. Iteration 01200, Avg-Loss: 1.9783, Accuracy: 0.2075\n",
      "Train-epoch 0. Iteration 01300, Avg-Loss: 1.9500, Accuracy: 0.2115\n",
      "Train-epoch 0. Iteration 01400, Avg-Loss: 1.9279, Accuracy: 0.2086\n",
      "Train-epoch 0. Iteration 01500, Avg-Loss: 1.9063, Accuracy: 0.2140\n",
      "Train-epoch 0. Iteration 01600, Avg-Loss: 1.8925, Accuracy: 0.2144\n",
      "Train-epoch 0. Iteration 01700, Avg-Loss: 1.8799, Accuracy: 0.2182\n",
      "Train-epoch 0. Iteration 01800, Avg-Loss: 1.8658, Accuracy: 0.2189\n",
      "Train-epoch 0. Iteration 01900, Avg-Loss: 1.8525, Accuracy: 0.2195\n",
      "Train-epoch 0. Iteration 02000, Avg-Loss: 1.8393, Accuracy: 0.2220\n",
      "Train-epoch 0. Iteration 02100, Avg-Loss: 1.8291, Accuracy: 0.2219\n",
      "Train-epoch 0. Iteration 02200, Avg-Loss: 1.8196, Accuracy: 0.2205\n",
      "Train-epoch 0. Iteration 02300, Avg-Loss: 1.8105, Accuracy: 0.2226\n",
      "Train-epoch 0. Iteration 02400, Avg-Loss: 1.8024, Accuracy: 0.2200\n",
      "Train-epoch 0. Iteration 02500, Avg-Loss: 1.7955, Accuracy: 0.2208\n",
      "Train-epoch 0. Iteration 02600, Avg-Loss: 1.7891, Accuracy: 0.2173\n",
      "Train-epoch 0. Iteration 02700, Avg-Loss: 1.7832, Accuracy: 0.2167\n",
      "Train-epoch 0. Iteration 02800, Avg-Loss: 1.7771, Accuracy: 0.2186\n",
      "Train-epoch 0. Iteration 02900, Avg-Loss: 1.7719, Accuracy: 0.2169\n",
      "Train-epoch 0. Iteration 03000, Avg-Loss: 1.7661, Accuracy: 0.2173\n",
      "Train-epoch 0. Iteration 03100, Avg-Loss: 1.7612, Accuracy: 0.2174\n",
      "Train-epoch 0. Iteration 03200, Avg-Loss: 1.7574, Accuracy: 0.2162\n",
      "Train-epoch 0. Iteration 03300, Avg-Loss: 1.7525, Accuracy: 0.2185\n",
      "Train-epoch 0. Iteration 03400, Avg-Loss: 1.7475, Accuracy: 0.2179\n",
      "Train-epoch 0. Iteration 03500, Avg-Loss: 1.7434, Accuracy: 0.2177\n",
      "Train-epoch 0. Iteration 03600, Avg-Loss: 1.7403, Accuracy: 0.2167\n",
      "Train-epoch 0. Iteration 03700, Avg-Loss: 1.7369, Accuracy: 0.2170\n",
      "Train-epoch 0. Iteration 03800, Avg-Loss: 1.7330, Accuracy: 0.2187\n",
      "Validation-epoch 0. Iteration 02591, Avg-Loss: 1.6132, Accuracy: 0.2120\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 1.6103, Accuracy: 0.2700\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 1.6128, Accuracy: 0.2350\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 1.6032, Accuracy: 0.2400\n",
      "Train-epoch 1. Iteration 00400, Avg-Loss: 1.6007, Accuracy: 0.2500\n",
      "Train-epoch 1. Iteration 00500, Avg-Loss: 1.6037, Accuracy: 0.2360\n",
      "Train-epoch 1. Iteration 00600, Avg-Loss: 1.6054, Accuracy: 0.2333\n",
      "Train-epoch 1. Iteration 00700, Avg-Loss: 1.6056, Accuracy: 0.2314\n",
      "Train-epoch 1. Iteration 00800, Avg-Loss: 1.6067, Accuracy: 0.2263\n",
      "Train-epoch 1. Iteration 00900, Avg-Loss: 1.6074, Accuracy: 0.2244\n",
      "Train-epoch 1. Iteration 01000, Avg-Loss: 1.6078, Accuracy: 0.2170\n",
      "Train-epoch 1. Iteration 01100, Avg-Loss: 1.6087, Accuracy: 0.2118\n",
      "Train-epoch 1. Iteration 01200, Avg-Loss: 1.6044, Accuracy: 0.2183\n",
      "Train-epoch 1. Iteration 01300, Avg-Loss: 1.6066, Accuracy: 0.2185\n",
      "Train-epoch 1. Iteration 01400, Avg-Loss: 1.6076, Accuracy: 0.2207\n",
      "Train-epoch 1. Iteration 01500, Avg-Loss: 1.6079, Accuracy: 0.2200\n",
      "Train-epoch 1. Iteration 01600, Avg-Loss: 1.6069, Accuracy: 0.2194\n",
      "Train-epoch 1. Iteration 01700, Avg-Loss: 1.6070, Accuracy: 0.2241\n",
      "Train-epoch 1. Iteration 01800, Avg-Loss: 1.6066, Accuracy: 0.2250\n",
      "Train-epoch 1. Iteration 01900, Avg-Loss: 1.6067, Accuracy: 0.2263\n",
      "Train-epoch 1. Iteration 02000, Avg-Loss: 1.6072, Accuracy: 0.2265\n",
      "Train-epoch 1. Iteration 02100, Avg-Loss: 1.6069, Accuracy: 0.2276\n",
      "Train-epoch 1. Iteration 02200, Avg-Loss: 1.6055, Accuracy: 0.2314\n",
      "Train-epoch 1. Iteration 02300, Avg-Loss: 1.6064, Accuracy: 0.2309\n",
      "Train-epoch 1. Iteration 02400, Avg-Loss: 1.6055, Accuracy: 0.2329\n",
      "Train-epoch 1. Iteration 02500, Avg-Loss: 1.6069, Accuracy: 0.2308\n",
      "Train-epoch 1. Iteration 02600, Avg-Loss: 1.6068, Accuracy: 0.2323\n",
      "Train-epoch 1. Iteration 02700, Avg-Loss: 1.6057, Accuracy: 0.2341\n",
      "Train-epoch 1. Iteration 02800, Avg-Loss: 1.6062, Accuracy: 0.2339\n",
      "Train-epoch 1. Iteration 02900, Avg-Loss: 1.6052, Accuracy: 0.2362\n",
      "Train-epoch 1. Iteration 03000, Avg-Loss: 1.6043, Accuracy: 0.2380\n",
      "Train-epoch 1. Iteration 03100, Avg-Loss: 1.6030, Accuracy: 0.2390\n",
      "Train-epoch 1. Iteration 03200, Avg-Loss: 1.6011, Accuracy: 0.2425\n",
      "Train-epoch 1. Iteration 03300, Avg-Loss: 1.6003, Accuracy: 0.2421\n",
      "Train-epoch 1. Iteration 03400, Avg-Loss: 1.5990, Accuracy: 0.2444\n",
      "Train-epoch 1. Iteration 03500, Avg-Loss: 1.5971, Accuracy: 0.2460\n",
      "Train-epoch 1. Iteration 03600, Avg-Loss: 1.5940, Accuracy: 0.2481\n",
      "Train-epoch 1. Iteration 03700, Avg-Loss: 1.5909, Accuracy: 0.2508\n",
      "Train-epoch 1. Iteration 03800, Avg-Loss: 1.5887, Accuracy: 0.2537\n",
      "Validation-epoch 1. Iteration 02591, Avg-Loss: 1.5717, Accuracy: 0.2151\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 1.5328, Accuracy: 0.3300\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 1.4496, Accuracy: 0.3650\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 1.4592, Accuracy: 0.3567\n",
      "Train-epoch 2. Iteration 00400, Avg-Loss: 1.4597, Accuracy: 0.3575\n",
      "Train-epoch 2. Iteration 00500, Avg-Loss: 1.4746, Accuracy: 0.3460\n",
      "Train-epoch 2. Iteration 00600, Avg-Loss: 1.4643, Accuracy: 0.3567\n",
      "Train-epoch 2. Iteration 00700, Avg-Loss: 1.4685, Accuracy: 0.3457\n",
      "Train-epoch 2. Iteration 00800, Avg-Loss: 1.4618, Accuracy: 0.3475\n",
      "Train-epoch 2. Iteration 00900, Avg-Loss: 1.4582, Accuracy: 0.3511\n",
      "Train-epoch 2. Iteration 01000, Avg-Loss: 1.4548, Accuracy: 0.3530\n",
      "Train-epoch 2. Iteration 01100, Avg-Loss: 1.4660, Accuracy: 0.3436\n",
      "Train-epoch 2. Iteration 01200, Avg-Loss: 1.4600, Accuracy: 0.3442\n",
      "Train-epoch 2. Iteration 01300, Avg-Loss: 1.4542, Accuracy: 0.3485\n",
      "Train-epoch 2. Iteration 01400, Avg-Loss: 1.4549, Accuracy: 0.3457\n",
      "Train-epoch 2. Iteration 01500, Avg-Loss: 1.4534, Accuracy: 0.3473\n",
      "Train-epoch 2. Iteration 01600, Avg-Loss: 1.4442, Accuracy: 0.3519\n",
      "Train-epoch 2. Iteration 01700, Avg-Loss: 1.4366, Accuracy: 0.3582\n",
      "Train-epoch 2. Iteration 01800, Avg-Loss: 1.4417, Accuracy: 0.3583\n",
      "Train-epoch 2. Iteration 01900, Avg-Loss: 1.4386, Accuracy: 0.3600\n",
      "Train-epoch 2. Iteration 02000, Avg-Loss: 1.4396, Accuracy: 0.3580\n",
      "Train-epoch 2. Iteration 02100, Avg-Loss: 1.4366, Accuracy: 0.3643\n",
      "Train-epoch 2. Iteration 02200, Avg-Loss: 1.4306, Accuracy: 0.3664\n",
      "Train-epoch 2. Iteration 02300, Avg-Loss: 1.4326, Accuracy: 0.3670\n",
      "Train-epoch 2. Iteration 02400, Avg-Loss: 1.4305, Accuracy: 0.3692\n",
      "Train-epoch 2. Iteration 02500, Avg-Loss: 1.4255, Accuracy: 0.3720\n",
      "Train-epoch 2. Iteration 02600, Avg-Loss: 1.4188, Accuracy: 0.3754\n",
      "Train-epoch 2. Iteration 02700, Avg-Loss: 1.4140, Accuracy: 0.3778\n",
      "Train-epoch 2. Iteration 02800, Avg-Loss: 1.4154, Accuracy: 0.3768\n",
      "Train-epoch 2. Iteration 02900, Avg-Loss: 1.4082, Accuracy: 0.3800\n",
      "Train-epoch 2. Iteration 03000, Avg-Loss: 1.4036, Accuracy: 0.3823\n",
      "Train-epoch 2. Iteration 03100, Avg-Loss: 1.3981, Accuracy: 0.3855\n",
      "Train-epoch 2. Iteration 03200, Avg-Loss: 1.3946, Accuracy: 0.3850\n",
      "Train-epoch 2. Iteration 03300, Avg-Loss: 1.3952, Accuracy: 0.3836\n",
      "Train-epoch 2. Iteration 03400, Avg-Loss: 1.3971, Accuracy: 0.3815\n",
      "Train-epoch 2. Iteration 03500, Avg-Loss: 1.3940, Accuracy: 0.3831\n",
      "Train-epoch 2. Iteration 03600, Avg-Loss: 1.3919, Accuracy: 0.3819\n",
      "Train-epoch 2. Iteration 03700, Avg-Loss: 1.3904, Accuracy: 0.3819\n",
      "Train-epoch 2. Iteration 03800, Avg-Loss: 1.3884, Accuracy: 0.3829\n",
      "Validation-epoch 2. Iteration 02591, Avg-Loss: 1.8152, Accuracy: 0.1347\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 1.3298, Accuracy: 0.4100\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 1.2466, Accuracy: 0.4700\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 1.2796, Accuracy: 0.4633\n",
      "Train-epoch 3. Iteration 00400, Avg-Loss: 1.2600, Accuracy: 0.4625\n",
      "Train-epoch 3. Iteration 00500, Avg-Loss: 1.2686, Accuracy: 0.4540\n",
      "Train-epoch 3. Iteration 00600, Avg-Loss: 1.2504, Accuracy: 0.4633\n",
      "Train-epoch 3. Iteration 00700, Avg-Loss: 1.2483, Accuracy: 0.4571\n",
      "Train-epoch 3. Iteration 00800, Avg-Loss: 1.2348, Accuracy: 0.4562\n",
      "Train-epoch 3. Iteration 00900, Avg-Loss: 1.2274, Accuracy: 0.4578\n",
      "Train-epoch 3. Iteration 01000, Avg-Loss: 1.2239, Accuracy: 0.4600\n",
      "Train-epoch 3. Iteration 01100, Avg-Loss: 1.2192, Accuracy: 0.4627\n",
      "Train-epoch 3. Iteration 01200, Avg-Loss: 1.2267, Accuracy: 0.4592\n",
      "Train-epoch 3. Iteration 01300, Avg-Loss: 1.2264, Accuracy: 0.4523\n",
      "Train-epoch 3. Iteration 01400, Avg-Loss: 1.2341, Accuracy: 0.4479\n",
      "Train-epoch 3. Iteration 01500, Avg-Loss: 1.2379, Accuracy: 0.4427\n",
      "Train-epoch 3. Iteration 01600, Avg-Loss: 1.2408, Accuracy: 0.4419\n",
      "Train-epoch 3. Iteration 01700, Avg-Loss: 1.2431, Accuracy: 0.4359\n",
      "Train-epoch 3. Iteration 01800, Avg-Loss: 1.2420, Accuracy: 0.4383\n",
      "Train-epoch 3. Iteration 01900, Avg-Loss: 1.2394, Accuracy: 0.4358\n",
      "Train-epoch 3. Iteration 02000, Avg-Loss: 1.2407, Accuracy: 0.4345\n",
      "Train-epoch 3. Iteration 02100, Avg-Loss: 1.2426, Accuracy: 0.4357\n",
      "Train-epoch 3. Iteration 02200, Avg-Loss: 1.2457, Accuracy: 0.4332\n",
      "Train-epoch 3. Iteration 02300, Avg-Loss: 1.2380, Accuracy: 0.4365\n",
      "Train-epoch 3. Iteration 02400, Avg-Loss: 1.2399, Accuracy: 0.4371\n",
      "Train-epoch 3. Iteration 02500, Avg-Loss: 1.2413, Accuracy: 0.4388\n",
      "Train-epoch 3. Iteration 02600, Avg-Loss: 1.2421, Accuracy: 0.4388\n",
      "Train-epoch 3. Iteration 02700, Avg-Loss: 1.2454, Accuracy: 0.4389\n",
      "Train-epoch 3. Iteration 02800, Avg-Loss: 1.2431, Accuracy: 0.4382\n",
      "Train-epoch 3. Iteration 02900, Avg-Loss: 1.2430, Accuracy: 0.4403\n",
      "Train-epoch 3. Iteration 03000, Avg-Loss: 1.2399, Accuracy: 0.4400\n",
      "Train-epoch 3. Iteration 03100, Avg-Loss: 1.2371, Accuracy: 0.4416\n",
      "Train-epoch 3. Iteration 03200, Avg-Loss: 1.2328, Accuracy: 0.4459\n",
      "Train-epoch 3. Iteration 03300, Avg-Loss: 1.2313, Accuracy: 0.4464\n",
      "Train-epoch 3. Iteration 03400, Avg-Loss: 1.2311, Accuracy: 0.4456\n",
      "Train-epoch 3. Iteration 03500, Avg-Loss: 1.2341, Accuracy: 0.4420\n",
      "Train-epoch 3. Iteration 03600, Avg-Loss: 1.2327, Accuracy: 0.4417\n",
      "Train-epoch 3. Iteration 03700, Avg-Loss: 1.2339, Accuracy: 0.4414\n",
      "Train-epoch 3. Iteration 03800, Avg-Loss: 1.2328, Accuracy: 0.4434\n",
      "Validation-epoch 3. Iteration 02591, Avg-Loss: 1.8448, Accuracy: 0.2907\n",
      "Train-epoch 4. Iteration 00100, Avg-Loss: 1.0995, Accuracy: 0.5600\n",
      "Train-epoch 4. Iteration 00200, Avg-Loss: 1.1125, Accuracy: 0.5100\n",
      "Train-epoch 4. Iteration 00300, Avg-Loss: 1.1373, Accuracy: 0.4767\n",
      "Train-epoch 4. Iteration 00400, Avg-Loss: 1.1228, Accuracy: 0.4800\n",
      "Train-epoch 4. Iteration 00500, Avg-Loss: 1.1221, Accuracy: 0.4840\n",
      "Train-epoch 4. Iteration 00600, Avg-Loss: 1.1490, Accuracy: 0.4683\n",
      "Train-epoch 4. Iteration 00700, Avg-Loss: 1.1473, Accuracy: 0.4671\n",
      "Train-epoch 4. Iteration 00800, Avg-Loss: 1.1568, Accuracy: 0.4713\n",
      "Train-epoch 4. Iteration 00900, Avg-Loss: 1.1482, Accuracy: 0.4789\n",
      "Train-epoch 4. Iteration 01000, Avg-Loss: 1.1327, Accuracy: 0.4940\n",
      "Train-epoch 4. Iteration 01100, Avg-Loss: 1.1242, Accuracy: 0.4982\n",
      "Train-epoch 4. Iteration 01200, Avg-Loss: 1.1165, Accuracy: 0.5050\n",
      "Train-epoch 4. Iteration 01300, Avg-Loss: 1.1257, Accuracy: 0.4992\n",
      "Train-epoch 4. Iteration 01400, Avg-Loss: 1.1292, Accuracy: 0.5007\n",
      "Train-epoch 4. Iteration 01500, Avg-Loss: 1.1275, Accuracy: 0.5087\n",
      "Train-epoch 4. Iteration 01600, Avg-Loss: 1.1274, Accuracy: 0.5069\n",
      "Train-epoch 4. Iteration 01700, Avg-Loss: 1.1234, Accuracy: 0.5071\n",
      "Train-epoch 4. Iteration 01800, Avg-Loss: 1.1257, Accuracy: 0.5089\n",
      "Train-epoch 4. Iteration 01900, Avg-Loss: 1.1259, Accuracy: 0.5074\n",
      "Train-epoch 4. Iteration 02000, Avg-Loss: 1.1170, Accuracy: 0.5125\n",
      "Train-epoch 4. Iteration 02100, Avg-Loss: 1.1177, Accuracy: 0.5143\n",
      "Train-epoch 4. Iteration 02200, Avg-Loss: 1.1155, Accuracy: 0.5136\n",
      "Train-epoch 4. Iteration 02300, Avg-Loss: 1.1074, Accuracy: 0.5187\n",
      "Train-epoch 4. Iteration 02400, Avg-Loss: 1.1071, Accuracy: 0.5208\n",
      "Train-epoch 4. Iteration 02500, Avg-Loss: 1.1048, Accuracy: 0.5196\n",
      "Train-epoch 4. Iteration 02600, Avg-Loss: 1.1003, Accuracy: 0.5219\n",
      "Train-epoch 4. Iteration 02700, Avg-Loss: 1.0971, Accuracy: 0.5219\n",
      "Train-epoch 4. Iteration 02800, Avg-Loss: 1.0956, Accuracy: 0.5225\n",
      "Train-epoch 4. Iteration 02900, Avg-Loss: 1.0959, Accuracy: 0.5228\n",
      "Train-epoch 4. Iteration 03000, Avg-Loss: 1.0936, Accuracy: 0.5237\n",
      "Train-epoch 4. Iteration 03100, Avg-Loss: 1.0911, Accuracy: 0.5242\n",
      "Train-epoch 4. Iteration 03200, Avg-Loss: 1.0918, Accuracy: 0.5238\n",
      "Train-epoch 4. Iteration 03300, Avg-Loss: 1.0930, Accuracy: 0.5230\n",
      "Train-epoch 4. Iteration 03400, Avg-Loss: 1.0915, Accuracy: 0.5244\n",
      "Train-epoch 4. Iteration 03500, Avg-Loss: 1.0901, Accuracy: 0.5240\n",
      "Train-epoch 4. Iteration 03600, Avg-Loss: 1.0863, Accuracy: 0.5253\n",
      "Train-epoch 4. Iteration 03700, Avg-Loss: 1.0860, Accuracy: 0.5243\n",
      "Train-epoch 4. Iteration 03800, Avg-Loss: 1.0861, Accuracy: 0.5242\n",
      "Validation-epoch 4. Iteration 02591, Avg-Loss: 3.0857, Accuracy: 0.2274\n",
      "Train-epoch 5. Iteration 00100, Avg-Loss: 0.9182, Accuracy: 0.5900\n",
      "Train-epoch 5. Iteration 00200, Avg-Loss: 0.9729, Accuracy: 0.5350\n",
      "Train-epoch 5. Iteration 00300, Avg-Loss: 0.9924, Accuracy: 0.5500\n",
      "Train-epoch 5. Iteration 00400, Avg-Loss: 1.0132, Accuracy: 0.5275\n",
      "Train-epoch 5. Iteration 00500, Avg-Loss: 1.0103, Accuracy: 0.5380\n",
      "Train-epoch 5. Iteration 00600, Avg-Loss: 1.0203, Accuracy: 0.5350\n",
      "Train-epoch 5. Iteration 00700, Avg-Loss: 1.0217, Accuracy: 0.5443\n",
      "Train-epoch 5. Iteration 00800, Avg-Loss: 1.0203, Accuracy: 0.5525\n",
      "Train-epoch 5. Iteration 00900, Avg-Loss: 1.0137, Accuracy: 0.5544\n",
      "Train-epoch 5. Iteration 01000, Avg-Loss: 1.0071, Accuracy: 0.5570\n",
      "Train-epoch 5. Iteration 01100, Avg-Loss: 1.0034, Accuracy: 0.5564\n",
      "Train-epoch 5. Iteration 01200, Avg-Loss: 0.9934, Accuracy: 0.5617\n",
      "Train-epoch 5. Iteration 01300, Avg-Loss: 0.9819, Accuracy: 0.5677\n",
      "Train-epoch 5. Iteration 01400, Avg-Loss: 0.9768, Accuracy: 0.5693\n",
      "Train-epoch 5. Iteration 01500, Avg-Loss: 0.9721, Accuracy: 0.5713\n",
      "Train-epoch 5. Iteration 01600, Avg-Loss: 0.9750, Accuracy: 0.5713\n",
      "Train-epoch 5. Iteration 01700, Avg-Loss: 0.9742, Accuracy: 0.5729\n",
      "Train-epoch 5. Iteration 01800, Avg-Loss: 0.9744, Accuracy: 0.5750\n",
      "Train-epoch 5. Iteration 01900, Avg-Loss: 0.9747, Accuracy: 0.5758\n",
      "Train-epoch 5. Iteration 02000, Avg-Loss: 0.9701, Accuracy: 0.5800\n",
      "Train-epoch 5. Iteration 02100, Avg-Loss: 0.9721, Accuracy: 0.5795\n",
      "Train-epoch 5. Iteration 02200, Avg-Loss: 0.9733, Accuracy: 0.5750\n",
      "Train-epoch 5. Iteration 02300, Avg-Loss: 0.9694, Accuracy: 0.5743\n",
      "Train-epoch 5. Iteration 02400, Avg-Loss: 0.9657, Accuracy: 0.5725\n",
      "Train-epoch 5. Iteration 02500, Avg-Loss: 0.9611, Accuracy: 0.5748\n",
      "Train-epoch 5. Iteration 02600, Avg-Loss: 0.9599, Accuracy: 0.5758\n",
      "Train-epoch 5. Iteration 02700, Avg-Loss: 0.9564, Accuracy: 0.5741\n",
      "Train-epoch 5. Iteration 02800, Avg-Loss: 0.9512, Accuracy: 0.5771\n",
      "Train-epoch 5. Iteration 02900, Avg-Loss: 0.9537, Accuracy: 0.5786\n",
      "Train-epoch 5. Iteration 03000, Avg-Loss: 0.9490, Accuracy: 0.5817\n",
      "Train-epoch 5. Iteration 03100, Avg-Loss: 0.9486, Accuracy: 0.5797\n",
      "Train-epoch 5. Iteration 03200, Avg-Loss: 0.9479, Accuracy: 0.5822\n",
      "Train-epoch 5. Iteration 03300, Avg-Loss: 0.9482, Accuracy: 0.5818\n",
      "Train-epoch 5. Iteration 03400, Avg-Loss: 0.9425, Accuracy: 0.5847\n",
      "Train-epoch 5. Iteration 03500, Avg-Loss: 0.9412, Accuracy: 0.5863\n",
      "Train-epoch 5. Iteration 03600, Avg-Loss: 0.9368, Accuracy: 0.5889\n",
      "Train-epoch 5. Iteration 03700, Avg-Loss: 0.9342, Accuracy: 0.5895\n",
      "Train-epoch 5. Iteration 03800, Avg-Loss: 0.9316, Accuracy: 0.5900\n",
      "Validation-epoch 5. Iteration 02591, Avg-Loss: 3.2668, Accuracy: 0.2154\n",
      "Train-epoch 6. Iteration 00100, Avg-Loss: 1.0323, Accuracy: 0.5700\n",
      "Train-epoch 6. Iteration 00200, Avg-Loss: 0.9262, Accuracy: 0.6100\n",
      "Train-epoch 6. Iteration 00300, Avg-Loss: 0.9229, Accuracy: 0.6133\n",
      "Train-epoch 6. Iteration 00400, Avg-Loss: 0.8866, Accuracy: 0.6350\n",
      "Train-epoch 6. Iteration 00500, Avg-Loss: 0.8587, Accuracy: 0.6240\n",
      "Train-epoch 6. Iteration 00600, Avg-Loss: 0.8771, Accuracy: 0.6267\n",
      "Train-epoch 6. Iteration 00700, Avg-Loss: 0.8853, Accuracy: 0.6243\n",
      "Train-epoch 6. Iteration 00800, Avg-Loss: 0.8812, Accuracy: 0.6250\n",
      "Train-epoch 6. Iteration 00900, Avg-Loss: 0.8877, Accuracy: 0.6167\n",
      "Train-epoch 6. Iteration 01000, Avg-Loss: 0.8781, Accuracy: 0.6150\n",
      "Train-epoch 6. Iteration 01100, Avg-Loss: 0.8808, Accuracy: 0.6173\n",
      "Train-epoch 6. Iteration 01200, Avg-Loss: 0.8806, Accuracy: 0.6167\n",
      "Train-epoch 6. Iteration 01300, Avg-Loss: 0.8696, Accuracy: 0.6192\n",
      "Train-epoch 6. Iteration 01400, Avg-Loss: 0.8692, Accuracy: 0.6164\n",
      "Train-epoch 6. Iteration 01500, Avg-Loss: 0.8666, Accuracy: 0.6187\n",
      "Train-epoch 6. Iteration 01600, Avg-Loss: 0.8751, Accuracy: 0.6150\n",
      "Train-epoch 6. Iteration 01700, Avg-Loss: 0.8727, Accuracy: 0.6171\n",
      "Train-epoch 6. Iteration 01800, Avg-Loss: 0.8692, Accuracy: 0.6183\n",
      "Train-epoch 6. Iteration 01900, Avg-Loss: 0.8634, Accuracy: 0.6189\n",
      "Train-epoch 6. Iteration 02000, Avg-Loss: 0.8585, Accuracy: 0.6195\n",
      "Train-epoch 6. Iteration 02100, Avg-Loss: 0.8600, Accuracy: 0.6186\n",
      "Train-epoch 6. Iteration 02200, Avg-Loss: 0.8586, Accuracy: 0.6209\n",
      "Train-epoch 6. Iteration 02300, Avg-Loss: 0.8555, Accuracy: 0.6187\n",
      "Train-epoch 6. Iteration 02400, Avg-Loss: 0.8515, Accuracy: 0.6196\n",
      "Train-epoch 6. Iteration 02500, Avg-Loss: 0.8539, Accuracy: 0.6192\n",
      "Train-epoch 6. Iteration 02600, Avg-Loss: 0.8492, Accuracy: 0.6200\n",
      "Train-epoch 6. Iteration 02700, Avg-Loss: 0.8464, Accuracy: 0.6200\n",
      "Train-epoch 6. Iteration 02800, Avg-Loss: 0.8506, Accuracy: 0.6164\n",
      "Train-epoch 6. Iteration 02900, Avg-Loss: 0.8530, Accuracy: 0.6155\n",
      "Train-epoch 6. Iteration 03000, Avg-Loss: 0.8561, Accuracy: 0.6137\n",
      "Train-epoch 6. Iteration 03100, Avg-Loss: 0.8524, Accuracy: 0.6145\n",
      "Train-epoch 6. Iteration 03200, Avg-Loss: 0.8501, Accuracy: 0.6122\n",
      "Train-epoch 6. Iteration 03300, Avg-Loss: 0.8484, Accuracy: 0.6148\n",
      "Train-epoch 6. Iteration 03400, Avg-Loss: 0.8438, Accuracy: 0.6182\n",
      "Train-epoch 6. Iteration 03500, Avg-Loss: 0.8427, Accuracy: 0.6171\n",
      "Train-epoch 6. Iteration 03600, Avg-Loss: 0.8423, Accuracy: 0.6178\n",
      "Train-epoch 6. Iteration 03700, Avg-Loss: 0.8453, Accuracy: 0.6168\n",
      "Train-epoch 6. Iteration 03800, Avg-Loss: 0.8449, Accuracy: 0.6171\n",
      "Validation-epoch 6. Iteration 02591, Avg-Loss: 2.4261, Accuracy: 0.3471\n",
      "Train-epoch 7. Iteration 00100, Avg-Loss: 0.7621, Accuracy: 0.6800\n",
      "Train-epoch 7. Iteration 00200, Avg-Loss: 0.7546, Accuracy: 0.6300\n",
      "Train-epoch 7. Iteration 00300, Avg-Loss: 0.7529, Accuracy: 0.6233\n",
      "Train-epoch 7. Iteration 00400, Avg-Loss: 0.7672, Accuracy: 0.6400\n",
      "Train-epoch 7. Iteration 00500, Avg-Loss: 0.7734, Accuracy: 0.6340\n",
      "Train-epoch 7. Iteration 00600, Avg-Loss: 0.7969, Accuracy: 0.6317\n",
      "Train-epoch 7. Iteration 00700, Avg-Loss: 0.8151, Accuracy: 0.6300\n",
      "Train-epoch 7. Iteration 00800, Avg-Loss: 0.8148, Accuracy: 0.6300\n",
      "Train-epoch 7. Iteration 00900, Avg-Loss: 0.8050, Accuracy: 0.6322\n",
      "Train-epoch 7. Iteration 01000, Avg-Loss: 0.7903, Accuracy: 0.6400\n",
      "Train-epoch 7. Iteration 01100, Avg-Loss: 0.7724, Accuracy: 0.6445\n",
      "Train-epoch 7. Iteration 01200, Avg-Loss: 0.7595, Accuracy: 0.6542\n",
      "Train-epoch 7. Iteration 01300, Avg-Loss: 0.7639, Accuracy: 0.6469\n",
      "Train-epoch 7. Iteration 01400, Avg-Loss: 0.7658, Accuracy: 0.6429\n",
      "Train-epoch 7. Iteration 01500, Avg-Loss: 0.7805, Accuracy: 0.6387\n",
      "Train-epoch 7. Iteration 01600, Avg-Loss: 0.7922, Accuracy: 0.6331\n",
      "Train-epoch 7. Iteration 01700, Avg-Loss: 0.7943, Accuracy: 0.6300\n",
      "Train-epoch 7. Iteration 01800, Avg-Loss: 0.7951, Accuracy: 0.6278\n",
      "Train-epoch 7. Iteration 01900, Avg-Loss: 0.7996, Accuracy: 0.6221\n",
      "Train-epoch 7. Iteration 02000, Avg-Loss: 0.7978, Accuracy: 0.6230\n",
      "Train-epoch 7. Iteration 02100, Avg-Loss: 0.7984, Accuracy: 0.6243\n",
      "Train-epoch 7. Iteration 02200, Avg-Loss: 0.8036, Accuracy: 0.6264\n",
      "Train-epoch 7. Iteration 02300, Avg-Loss: 0.8035, Accuracy: 0.6270\n",
      "Train-epoch 7. Iteration 02400, Avg-Loss: 0.8006, Accuracy: 0.6304\n",
      "Train-epoch 7. Iteration 02500, Avg-Loss: 0.8075, Accuracy: 0.6260\n",
      "Train-epoch 7. Iteration 02600, Avg-Loss: 0.8060, Accuracy: 0.6269\n",
      "Train-epoch 7. Iteration 02700, Avg-Loss: 0.8030, Accuracy: 0.6274\n",
      "Train-epoch 7. Iteration 02800, Avg-Loss: 0.8007, Accuracy: 0.6286\n",
      "Train-epoch 7. Iteration 02900, Avg-Loss: 0.7998, Accuracy: 0.6283\n",
      "Train-epoch 7. Iteration 03000, Avg-Loss: 0.7997, Accuracy: 0.6297\n",
      "Train-epoch 7. Iteration 03100, Avg-Loss: 0.7955, Accuracy: 0.6316\n",
      "Train-epoch 7. Iteration 03200, Avg-Loss: 0.7942, Accuracy: 0.6328\n",
      "Train-epoch 7. Iteration 03300, Avg-Loss: 0.7941, Accuracy: 0.6321\n",
      "Train-epoch 7. Iteration 03400, Avg-Loss: 0.7993, Accuracy: 0.6309\n",
      "Train-epoch 7. Iteration 03500, Avg-Loss: 0.7942, Accuracy: 0.6337\n",
      "Train-epoch 7. Iteration 03600, Avg-Loss: 0.7919, Accuracy: 0.6339\n",
      "Train-epoch 7. Iteration 03700, Avg-Loss: 0.7905, Accuracy: 0.6343\n",
      "Train-epoch 7. Iteration 03800, Avg-Loss: 0.7921, Accuracy: 0.6321\n",
      "Validation-epoch 7. Iteration 02591, Avg-Loss: 3.9330, Accuracy: 0.3058\n",
      "Train-epoch 8. Iteration 00100, Avg-Loss: 0.8026, Accuracy: 0.5900\n",
      "Train-epoch 8. Iteration 00200, Avg-Loss: 0.8356, Accuracy: 0.5800\n",
      "Train-epoch 8. Iteration 00300, Avg-Loss: 0.8164, Accuracy: 0.5933\n",
      "Train-epoch 8. Iteration 00400, Avg-Loss: 0.7806, Accuracy: 0.6175\n",
      "Train-epoch 8. Iteration 00500, Avg-Loss: 0.7769, Accuracy: 0.6260\n",
      "Train-epoch 8. Iteration 00600, Avg-Loss: 0.7601, Accuracy: 0.6317\n",
      "Train-epoch 8. Iteration 00700, Avg-Loss: 0.7518, Accuracy: 0.6257\n",
      "Train-epoch 8. Iteration 00800, Avg-Loss: 0.7389, Accuracy: 0.6312\n",
      "Train-epoch 8. Iteration 00900, Avg-Loss: 0.7447, Accuracy: 0.6267\n",
      "Train-epoch 8. Iteration 01000, Avg-Loss: 0.7395, Accuracy: 0.6290\n",
      "Train-epoch 8. Iteration 01100, Avg-Loss: 0.7297, Accuracy: 0.6373\n",
      "Train-epoch 8. Iteration 01200, Avg-Loss: 0.7386, Accuracy: 0.6375\n",
      "Train-epoch 8. Iteration 01300, Avg-Loss: 0.7409, Accuracy: 0.6354\n",
      "Train-epoch 8. Iteration 01400, Avg-Loss: 0.7613, Accuracy: 0.6257\n",
      "Train-epoch 8. Iteration 01500, Avg-Loss: 0.7540, Accuracy: 0.6313\n",
      "Train-epoch 8. Iteration 01600, Avg-Loss: 0.7521, Accuracy: 0.6381\n",
      "Train-epoch 8. Iteration 01700, Avg-Loss: 0.7502, Accuracy: 0.6412\n",
      "Train-epoch 8. Iteration 01800, Avg-Loss: 0.7522, Accuracy: 0.6411\n",
      "Train-epoch 8. Iteration 01900, Avg-Loss: 0.7491, Accuracy: 0.6437\n",
      "Train-epoch 8. Iteration 02000, Avg-Loss: 0.7464, Accuracy: 0.6435\n",
      "Train-epoch 8. Iteration 02100, Avg-Loss: 0.7423, Accuracy: 0.6481\n",
      "Train-epoch 8. Iteration 02200, Avg-Loss: 0.7430, Accuracy: 0.6464\n",
      "Train-epoch 8. Iteration 02300, Avg-Loss: 0.7448, Accuracy: 0.6470\n",
      "Train-epoch 8. Iteration 02400, Avg-Loss: 0.7411, Accuracy: 0.6483\n",
      "Train-epoch 8. Iteration 02500, Avg-Loss: 0.7478, Accuracy: 0.6460\n",
      "Train-epoch 8. Iteration 02600, Avg-Loss: 0.7521, Accuracy: 0.6438\n",
      "Train-epoch 8. Iteration 02700, Avg-Loss: 0.7602, Accuracy: 0.6433\n",
      "Train-epoch 8. Iteration 02800, Avg-Loss: 0.7572, Accuracy: 0.6436\n",
      "Train-epoch 8. Iteration 02900, Avg-Loss: 0.7554, Accuracy: 0.6452\n",
      "Train-epoch 8. Iteration 03000, Avg-Loss: 0.7544, Accuracy: 0.6447\n",
      "Train-epoch 8. Iteration 03100, Avg-Loss: 0.7512, Accuracy: 0.6442\n",
      "Train-epoch 8. Iteration 03200, Avg-Loss: 0.7506, Accuracy: 0.6434\n",
      "Train-epoch 8. Iteration 03300, Avg-Loss: 0.7511, Accuracy: 0.6424\n",
      "Train-epoch 8. Iteration 03400, Avg-Loss: 0.7446, Accuracy: 0.6468\n",
      "Train-epoch 8. Iteration 03500, Avg-Loss: 0.7451, Accuracy: 0.6457\n",
      "Train-epoch 8. Iteration 03600, Avg-Loss: 0.7452, Accuracy: 0.6469\n",
      "Train-epoch 8. Iteration 03700, Avg-Loss: 0.7445, Accuracy: 0.6473\n",
      "Train-epoch 8. Iteration 03800, Avg-Loss: 0.7429, Accuracy: 0.6495\n",
      "Validation-epoch 8. Iteration 02591, Avg-Loss: 4.0632, Accuracy: 0.2653\n",
      "Train-epoch 9. Iteration 00100, Avg-Loss: 0.6297, Accuracy: 0.6500\n",
      "Train-epoch 9. Iteration 00200, Avg-Loss: 0.6881, Accuracy: 0.6350\n",
      "Train-epoch 9. Iteration 00300, Avg-Loss: 0.6768, Accuracy: 0.6500\n",
      "Train-epoch 9. Iteration 00400, Avg-Loss: 0.6921, Accuracy: 0.6525\n",
      "Train-epoch 9. Iteration 00500, Avg-Loss: 0.6755, Accuracy: 0.6480\n",
      "Train-epoch 9. Iteration 00600, Avg-Loss: 0.6708, Accuracy: 0.6567\n",
      "Train-epoch 9. Iteration 00700, Avg-Loss: 0.6834, Accuracy: 0.6571\n",
      "Train-epoch 9. Iteration 00800, Avg-Loss: 0.6814, Accuracy: 0.6587\n",
      "Train-epoch 9. Iteration 00900, Avg-Loss: 0.6876, Accuracy: 0.6589\n",
      "Train-epoch 9. Iteration 01000, Avg-Loss: 0.6885, Accuracy: 0.6560\n",
      "Train-epoch 9. Iteration 01100, Avg-Loss: 0.6878, Accuracy: 0.6573\n",
      "Train-epoch 9. Iteration 01200, Avg-Loss: 0.6906, Accuracy: 0.6558\n",
      "Train-epoch 9. Iteration 01300, Avg-Loss: 0.6903, Accuracy: 0.6577\n",
      "Train-epoch 9. Iteration 01400, Avg-Loss: 0.6974, Accuracy: 0.6564\n",
      "Train-epoch 9. Iteration 01500, Avg-Loss: 0.6947, Accuracy: 0.6553\n",
      "Train-epoch 9. Iteration 01600, Avg-Loss: 0.6961, Accuracy: 0.6556\n",
      "Train-epoch 9. Iteration 01700, Avg-Loss: 0.6906, Accuracy: 0.6582\n",
      "Train-epoch 9. Iteration 01800, Avg-Loss: 0.6928, Accuracy: 0.6589\n",
      "Train-epoch 9. Iteration 01900, Avg-Loss: 0.6926, Accuracy: 0.6584\n",
      "Train-epoch 9. Iteration 02000, Avg-Loss: 0.6951, Accuracy: 0.6565\n",
      "Train-epoch 9. Iteration 02100, Avg-Loss: 0.6961, Accuracy: 0.6533\n",
      "Train-epoch 9. Iteration 02200, Avg-Loss: 0.6964, Accuracy: 0.6518\n",
      "Train-epoch 9. Iteration 02300, Avg-Loss: 0.7018, Accuracy: 0.6504\n",
      "Train-epoch 9. Iteration 02400, Avg-Loss: 0.7065, Accuracy: 0.6496\n",
      "Train-epoch 9. Iteration 02500, Avg-Loss: 0.6993, Accuracy: 0.6548\n",
      "Train-epoch 9. Iteration 02600, Avg-Loss: 0.7021, Accuracy: 0.6538\n",
      "Train-epoch 9. Iteration 02700, Avg-Loss: 0.7032, Accuracy: 0.6552\n",
      "Train-epoch 9. Iteration 02800, Avg-Loss: 0.7047, Accuracy: 0.6561\n",
      "Train-epoch 9. Iteration 02900, Avg-Loss: 0.7042, Accuracy: 0.6569\n",
      "Train-epoch 9. Iteration 03000, Avg-Loss: 0.7046, Accuracy: 0.6577\n",
      "Train-epoch 9. Iteration 03100, Avg-Loss: 0.7089, Accuracy: 0.6555\n",
      "Train-epoch 9. Iteration 03200, Avg-Loss: 0.7114, Accuracy: 0.6528\n",
      "Train-epoch 9. Iteration 03300, Avg-Loss: 0.7120, Accuracy: 0.6533\n",
      "Train-epoch 9. Iteration 03400, Avg-Loss: 0.7124, Accuracy: 0.6529\n",
      "Train-epoch 9. Iteration 03500, Avg-Loss: 0.7121, Accuracy: 0.6520\n",
      "Train-epoch 9. Iteration 03600, Avg-Loss: 0.7140, Accuracy: 0.6536\n",
      "Train-epoch 9. Iteration 03700, Avg-Loss: 0.7134, Accuracy: 0.6532\n",
      "Train-epoch 9. Iteration 03800, Avg-Loss: 0.7127, Accuracy: 0.6534\n",
      "Validation-epoch 9. Iteration 02591, Avg-Loss: 6.6668, Accuracy: 0.2270\n",
      "Train-epoch 10. Iteration 00100, Avg-Loss: 0.8179, Accuracy: 0.6300\n",
      "Train-epoch 10. Iteration 00200, Avg-Loss: 0.8389, Accuracy: 0.6200\n",
      "Train-epoch 10. Iteration 00300, Avg-Loss: 0.8076, Accuracy: 0.6100\n",
      "Train-epoch 10. Iteration 00400, Avg-Loss: 0.7752, Accuracy: 0.6250\n",
      "Train-epoch 10. Iteration 00500, Avg-Loss: 0.7721, Accuracy: 0.6340\n",
      "Train-epoch 10. Iteration 00600, Avg-Loss: 0.7652, Accuracy: 0.6367\n",
      "Train-epoch 10. Iteration 00700, Avg-Loss: 0.7391, Accuracy: 0.6486\n",
      "Train-epoch 10. Iteration 00800, Avg-Loss: 0.7285, Accuracy: 0.6600\n",
      "Train-epoch 10. Iteration 00900, Avg-Loss: 0.7314, Accuracy: 0.6556\n",
      "Train-epoch 10. Iteration 01000, Avg-Loss: 0.7268, Accuracy: 0.6590\n",
      "Train-epoch 10. Iteration 01100, Avg-Loss: 0.7157, Accuracy: 0.6573\n",
      "Train-epoch 10. Iteration 01200, Avg-Loss: 0.7130, Accuracy: 0.6575\n",
      "Train-epoch 10. Iteration 01300, Avg-Loss: 0.7126, Accuracy: 0.6600\n",
      "Train-epoch 10. Iteration 01400, Avg-Loss: 0.7094, Accuracy: 0.6614\n",
      "Train-epoch 10. Iteration 01500, Avg-Loss: 0.7060, Accuracy: 0.6627\n",
      "Train-epoch 10. Iteration 01600, Avg-Loss: 0.7013, Accuracy: 0.6663\n",
      "Train-epoch 10. Iteration 01700, Avg-Loss: 0.7020, Accuracy: 0.6641\n",
      "Train-epoch 10. Iteration 01800, Avg-Loss: 0.7071, Accuracy: 0.6583\n",
      "Train-epoch 10. Iteration 01900, Avg-Loss: 0.7126, Accuracy: 0.6568\n",
      "Train-epoch 10. Iteration 02000, Avg-Loss: 0.7195, Accuracy: 0.6505\n",
      "Train-epoch 10. Iteration 02100, Avg-Loss: 0.7197, Accuracy: 0.6500\n",
      "Train-epoch 10. Iteration 02200, Avg-Loss: 0.7210, Accuracy: 0.6486\n",
      "Train-epoch 10. Iteration 02300, Avg-Loss: 0.7191, Accuracy: 0.6513\n",
      "Train-epoch 10. Iteration 02400, Avg-Loss: 0.7219, Accuracy: 0.6496\n",
      "Train-epoch 10. Iteration 02500, Avg-Loss: 0.7219, Accuracy: 0.6472\n",
      "Train-epoch 10. Iteration 02600, Avg-Loss: 0.7188, Accuracy: 0.6488\n",
      "Train-epoch 10. Iteration 02700, Avg-Loss: 0.7163, Accuracy: 0.6504\n",
      "Train-epoch 10. Iteration 02800, Avg-Loss: 0.7121, Accuracy: 0.6514\n",
      "Train-epoch 10. Iteration 02900, Avg-Loss: 0.7082, Accuracy: 0.6528\n",
      "Train-epoch 10. Iteration 03000, Avg-Loss: 0.7055, Accuracy: 0.6530\n",
      "Train-epoch 10. Iteration 03100, Avg-Loss: 0.7034, Accuracy: 0.6545\n",
      "Train-epoch 10. Iteration 03200, Avg-Loss: 0.7017, Accuracy: 0.6544\n",
      "Train-epoch 10. Iteration 03300, Avg-Loss: 0.6997, Accuracy: 0.6558\n",
      "Train-epoch 10. Iteration 03400, Avg-Loss: 0.7011, Accuracy: 0.6553\n",
      "Train-epoch 10. Iteration 03500, Avg-Loss: 0.7003, Accuracy: 0.6563\n",
      "Train-epoch 10. Iteration 03600, Avg-Loss: 0.6993, Accuracy: 0.6575\n",
      "Train-epoch 10. Iteration 03700, Avg-Loss: 0.6984, Accuracy: 0.6589\n",
      "Train-epoch 10. Iteration 03800, Avg-Loss: 0.7019, Accuracy: 0.6582\n",
      "Validation-epoch 10. Iteration 02591, Avg-Loss: 2.9894, Accuracy: 0.2764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6eaf4f175d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-4f07dd18389d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mframe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#scores = model(frame_list[:,0,:,:,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0a75b42e9b70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_3d)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_3d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# flatten output of conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "\n",
    "#my_model = models.resnet18(pretrained=True)\n",
    "#my_model.fc = nn.Linear(my_model.fc.in_features,categories)\n",
    "my_model = AverageModel(output_size=categories)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "epochs = 100\n",
    "\n",
    "train_model(my_model, loss_fn, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 1.6, PyTorch 0.4, Keras",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
