{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose(\n",
    "    [transforms.Resize(256),  # 1. Resize smallest side to 256.\n",
    "     transforms.CenterCrop(224), # 2. Crop center square of 224x224 pixels.\n",
    "     transforms.ToTensor(), # 3. Convert to pytorch tensor.\n",
    "     transforms.Normalize(mean = [0.485, 0.456, 0.406],  # normalize.\n",
    "                          std = [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list(cat,d1):\n",
    "    ret = list()\n",
    "    for frame in os.listdir('data/data_first_25/{}/{}'.format(cat,d1)):\n",
    "        img_pil = Image.open('data/data_first_25/{}/{}/{}'.format(cat,d1,frame))\n",
    "        input_img = test_transform(img_pil).unsqueeze(0)\n",
    "        ret.append(input_img)\n",
    "    return ret\n",
    "\n",
    "def createTrainAndValSet(categories,trainPercentage):\n",
    "    category_options = sorted(os.listdir('data/data_first_25'))\n",
    "    category_names = category_options[:categories]\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    i=0\n",
    "    for cat in category_names:\n",
    "        print(cat)\n",
    "        for d1 in os.listdir('data/data_first_25/{}'.format(cat)):\n",
    "            r = random.uniform(0,1)\n",
    "            img_list = get_img_list(cat,d1)\n",
    "            if int(d1[1:3]) <= trainPercentage * 25:\n",
    "                train_set.append((img_list,i))\n",
    "            else:\n",
    "                val_set.append((img_list,i))\n",
    "        i+=1\n",
    "    return train_set,val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageModel(nn.Module):\n",
    "    def __init__(self, output_size=5):\n",
    "        super(AverageModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.output_size = output_size\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, output_size)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        result = torch.zeros((1,self.output_size)).cuda()\n",
    "        \n",
    "        for t in range(x_3d.size(1)):\n",
    "            x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
    "            x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = self.fc1(x)\n",
    "          \n",
    "            result += x\n",
    "        result /= x_3d.size(1)\n",
    "        '''\n",
    "        x = self.resnet(x_3d[:, 0,:, :, :])  # ResNet\n",
    "        x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        result+=x\n",
    "        '''\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_accuracies = []; train_losses = [];\n",
    "val_accuracies = []; val_losses = [];\n",
    "\n",
    "def train_model(model, loss_fn, optimizer, epochs):\n",
    "    model = model.cuda()\n",
    "    loss_fn = loss_fn.cuda()\n",
    "    batchSize = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        cum_loss = 0\n",
    "\n",
    "        i = 0\n",
    "        model.train()\n",
    "        shuffle(train_set)\n",
    "        for video in train_set:\n",
    "            frame_list, target_cat = video\n",
    "            frame_list = torch.stack(frame_list, dim=0).transpose(0, 1).cuda()\n",
    "            #scores = model(frame_list[:,0,:,:,:])\n",
    "            scores = model(frame_list)\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(scores, torch.tensor(np.array([target_cat]),dtype=torch.long).cuda())\n",
    "            max_score, max_label = scores.max(1)\n",
    "            if max_label == target_cat:\n",
    "                correct+=1\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #print(model.fc1.weight)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                    (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
    "            i += 1\n",
    "            \n",
    "        train_accuracies.append(correct / len(train_set))\n",
    "        train_losses.append(cum_loss / (i + 1))   \n",
    "        \n",
    "        i = 0\n",
    "        correct = 0\n",
    "        cum_loss = 0\n",
    "        model.eval()\n",
    "        for video in val_set:\n",
    "            frame_list, target_cat = video\n",
    "            frame_list = torch.stack(frame_list, dim=0).transpose(0, 1).cuda()\n",
    "            #scores = model(frame_list[:,0,:,:,:])\n",
    "            scores = model(frame_list)\n",
    "            \n",
    "            loss = loss_fn(scores, torch.tensor(np.array([target_cat]),dtype=torch.long).cuda())\n",
    "            max_score, max_label = scores.max(1)\n",
    "            if max_label == target_cat:\n",
    "                correct+=1\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            i += 1\n",
    "        print('Validation-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "               (epoch, i + 1, cum_loss / (i + 1), correct / len(val_set)))\n",
    "        \n",
    "        val_accuracies.append(correct / len(val_set))\n",
    "        val_losses.append(cum_loss / (i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApplyEyeMakeup\n",
      "ApplyLipstick\n",
      "Archery\n",
      "BabyCrawling\n",
      "BalanceBeam\n"
     ]
    }
   ],
   "source": [
    "categories = 5\n",
    "\n",
    "train_set, val_set = createTrainAndValSet(categories, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00100, Avg-Loss: 3.4372, Accuracy: 0.1800\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 2.6521, Accuracy: 0.2050\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 2.3941, Accuracy: 0.2133\n",
      "Train-epoch 0. Iteration 00400, Avg-Loss: 2.2316, Accuracy: 0.2050\n",
      "Train-epoch 0. Iteration 00500, Avg-Loss: 2.1125, Accuracy: 0.2080\n",
      "Validation-epoch 0. Iteration 00134, Avg-Loss: 1.7190, Accuracy: 0.1654\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 1.6423, Accuracy: 0.2200\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 1.6201, Accuracy: 0.2450\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 1.6135, Accuracy: 0.2700\n",
      "Train-epoch 1. Iteration 00400, Avg-Loss: 1.6094, Accuracy: 0.2575\n",
      "Train-epoch 1. Iteration 00500, Avg-Loss: 1.6044, Accuracy: 0.2480\n",
      "Validation-epoch 1. Iteration 00134, Avg-Loss: 5.5990, Accuracy: 0.3534\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 1.5403, Accuracy: 0.2700\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 1.6206, Accuracy: 0.2400\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 1.6239, Accuracy: 0.2167\n",
      "Train-epoch 2. Iteration 00400, Avg-Loss: 1.6112, Accuracy: 0.2275\n",
      "Train-epoch 2. Iteration 00500, Avg-Loss: 1.6057, Accuracy: 0.2100\n",
      "Validation-epoch 2. Iteration 00134, Avg-Loss: 1.6072, Accuracy: 0.2406\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 1.5454, Accuracy: 0.3200\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 1.5394, Accuracy: 0.2950\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 1.5295, Accuracy: 0.3200\n",
      "Train-epoch 3. Iteration 00400, Avg-Loss: 1.5329, Accuracy: 0.3125\n",
      "Train-epoch 3. Iteration 00500, Avg-Loss: 1.5379, Accuracy: 0.2960\n",
      "Validation-epoch 3. Iteration 00134, Avg-Loss: 1.6673, Accuracy: 0.3684\n",
      "Train-epoch 4. Iteration 00100, Avg-Loss: 1.4736, Accuracy: 0.3400\n",
      "Train-epoch 4. Iteration 00200, Avg-Loss: 1.4348, Accuracy: 0.3650\n",
      "Train-epoch 4. Iteration 00300, Avg-Loss: 1.4342, Accuracy: 0.3600\n",
      "Train-epoch 4. Iteration 00400, Avg-Loss: 1.4324, Accuracy: 0.3650\n",
      "Train-epoch 4. Iteration 00500, Avg-Loss: 1.4052, Accuracy: 0.3700\n",
      "Validation-epoch 4. Iteration 00134, Avg-Loss: 1.7283, Accuracy: 0.2105\n",
      "Train-epoch 5. Iteration 00100, Avg-Loss: 1.2317, Accuracy: 0.5100\n",
      "Train-epoch 5. Iteration 00200, Avg-Loss: 1.2604, Accuracy: 0.4650\n",
      "Train-epoch 5. Iteration 00300, Avg-Loss: 1.2442, Accuracy: 0.4733\n",
      "Train-epoch 5. Iteration 00400, Avg-Loss: 1.2532, Accuracy: 0.4700\n",
      "Train-epoch 5. Iteration 00500, Avg-Loss: 1.2322, Accuracy: 0.4820\n",
      "Validation-epoch 5. Iteration 00134, Avg-Loss: 1.5540, Accuracy: 0.3609\n",
      "Train-epoch 6. Iteration 00100, Avg-Loss: 1.1868, Accuracy: 0.4500\n",
      "Train-epoch 6. Iteration 00200, Avg-Loss: 1.1606, Accuracy: 0.4700\n",
      "Train-epoch 6. Iteration 00300, Avg-Loss: 1.1006, Accuracy: 0.5167\n",
      "Train-epoch 6. Iteration 00400, Avg-Loss: 1.1166, Accuracy: 0.5125\n",
      "Train-epoch 6. Iteration 00500, Avg-Loss: 1.0838, Accuracy: 0.5360\n",
      "Validation-epoch 6. Iteration 00134, Avg-Loss: 1.4431, Accuracy: 0.2632\n",
      "Train-epoch 7. Iteration 00100, Avg-Loss: 1.0972, Accuracy: 0.5300\n",
      "Train-epoch 7. Iteration 00200, Avg-Loss: 1.0174, Accuracy: 0.5550\n",
      "Train-epoch 7. Iteration 00300, Avg-Loss: 0.9870, Accuracy: 0.5567\n",
      "Train-epoch 7. Iteration 00400, Avg-Loss: 0.9658, Accuracy: 0.5800\n",
      "Train-epoch 7. Iteration 00500, Avg-Loss: 0.9982, Accuracy: 0.5680\n",
      "Validation-epoch 7. Iteration 00134, Avg-Loss: 1.5765, Accuracy: 0.2331\n",
      "Train-epoch 8. Iteration 00100, Avg-Loss: 0.7493, Accuracy: 0.6600\n",
      "Train-epoch 8. Iteration 00200, Avg-Loss: 0.8733, Accuracy: 0.6400\n",
      "Train-epoch 8. Iteration 00300, Avg-Loss: 0.8577, Accuracy: 0.6367\n",
      "Train-epoch 8. Iteration 00400, Avg-Loss: 0.8706, Accuracy: 0.6425\n",
      "Train-epoch 8. Iteration 00500, Avg-Loss: 0.8886, Accuracy: 0.6360\n",
      "Validation-epoch 8. Iteration 00134, Avg-Loss: 2.2097, Accuracy: 0.3083\n",
      "Train-epoch 9. Iteration 00100, Avg-Loss: 0.7270, Accuracy: 0.6600\n",
      "Train-epoch 9. Iteration 00200, Avg-Loss: 0.8462, Accuracy: 0.6500\n",
      "Train-epoch 9. Iteration 00300, Avg-Loss: 0.7881, Accuracy: 0.6633\n",
      "Train-epoch 9. Iteration 00400, Avg-Loss: 0.7595, Accuracy: 0.6775\n",
      "Train-epoch 9. Iteration 00500, Avg-Loss: 0.7685, Accuracy: 0.6820\n",
      "Validation-epoch 9. Iteration 00134, Avg-Loss: 2.4874, Accuracy: 0.3233\n",
      "Train-epoch 10. Iteration 00100, Avg-Loss: 0.6349, Accuracy: 0.7200\n",
      "Train-epoch 10. Iteration 00200, Avg-Loss: 0.6545, Accuracy: 0.7150\n",
      "Train-epoch 10. Iteration 00300, Avg-Loss: 0.6932, Accuracy: 0.7000\n",
      "Train-epoch 10. Iteration 00400, Avg-Loss: 0.6815, Accuracy: 0.7025\n",
      "Train-epoch 10. Iteration 00500, Avg-Loss: 0.6849, Accuracy: 0.6900\n",
      "Validation-epoch 10. Iteration 00134, Avg-Loss: 2.6971, Accuracy: 0.2782\n",
      "Train-epoch 11. Iteration 00100, Avg-Loss: 0.6537, Accuracy: 0.7600\n",
      "Train-epoch 11. Iteration 00200, Avg-Loss: 0.6262, Accuracy: 0.7450\n",
      "Train-epoch 11. Iteration 00300, Avg-Loss: 0.6249, Accuracy: 0.7367\n",
      "Train-epoch 11. Iteration 00400, Avg-Loss: 0.5915, Accuracy: 0.7500\n",
      "Train-epoch 11. Iteration 00500, Avg-Loss: 0.6024, Accuracy: 0.7440\n",
      "Validation-epoch 11. Iteration 00134, Avg-Loss: 2.8199, Accuracy: 0.3383\n",
      "Train-epoch 12. Iteration 00100, Avg-Loss: 0.3953, Accuracy: 0.8600\n",
      "Train-epoch 12. Iteration 00200, Avg-Loss: 0.4912, Accuracy: 0.8150\n",
      "Train-epoch 12. Iteration 00300, Avg-Loss: 0.5475, Accuracy: 0.7933\n",
      "Train-epoch 12. Iteration 00400, Avg-Loss: 0.5215, Accuracy: 0.7975\n",
      "Train-epoch 12. Iteration 00500, Avg-Loss: 0.5236, Accuracy: 0.8040\n",
      "Validation-epoch 12. Iteration 00134, Avg-Loss: 2.4700, Accuracy: 0.3759\n",
      "Train-epoch 13. Iteration 00100, Avg-Loss: 0.4048, Accuracy: 0.8600\n",
      "Train-epoch 13. Iteration 00200, Avg-Loss: 0.3978, Accuracy: 0.8350\n",
      "Train-epoch 13. Iteration 00300, Avg-Loss: 0.4042, Accuracy: 0.8400\n",
      "Train-epoch 13. Iteration 00400, Avg-Loss: 0.4242, Accuracy: 0.8375\n",
      "Train-epoch 13. Iteration 00500, Avg-Loss: 0.4240, Accuracy: 0.8420\n",
      "Validation-epoch 13. Iteration 00134, Avg-Loss: 4.2735, Accuracy: 0.2707\n",
      "Train-epoch 14. Iteration 00100, Avg-Loss: 0.2683, Accuracy: 0.9300\n",
      "Train-epoch 14. Iteration 00200, Avg-Loss: 0.2515, Accuracy: 0.9300\n",
      "Train-epoch 14. Iteration 00300, Avg-Loss: 0.2506, Accuracy: 0.9200\n",
      "Train-epoch 14. Iteration 00400, Avg-Loss: 0.2756, Accuracy: 0.9100\n",
      "Train-epoch 14. Iteration 00500, Avg-Loss: 0.3155, Accuracy: 0.8900\n",
      "Validation-epoch 14. Iteration 00134, Avg-Loss: 3.9669, Accuracy: 0.2782\n",
      "Train-epoch 15. Iteration 00100, Avg-Loss: 0.3258, Accuracy: 0.8700\n",
      "Train-epoch 15. Iteration 00200, Avg-Loss: 0.2839, Accuracy: 0.8950\n",
      "Train-epoch 15. Iteration 00300, Avg-Loss: 0.2996, Accuracy: 0.8833\n",
      "Train-epoch 15. Iteration 00400, Avg-Loss: 0.3135, Accuracy: 0.8800\n",
      "Train-epoch 15. Iteration 00500, Avg-Loss: 0.2941, Accuracy: 0.8900\n",
      "Validation-epoch 15. Iteration 00134, Avg-Loss: 4.4547, Accuracy: 0.2932\n",
      "Train-epoch 16. Iteration 00100, Avg-Loss: 0.2511, Accuracy: 0.9300\n",
      "Train-epoch 16. Iteration 00200, Avg-Loss: 0.2454, Accuracy: 0.9350\n",
      "Train-epoch 16. Iteration 00300, Avg-Loss: 0.2173, Accuracy: 0.9433\n",
      "Train-epoch 16. Iteration 00400, Avg-Loss: 0.2051, Accuracy: 0.9475\n",
      "Train-epoch 16. Iteration 00500, Avg-Loss: 0.2194, Accuracy: 0.9340\n",
      "Validation-epoch 16. Iteration 00134, Avg-Loss: 2.9759, Accuracy: 0.3609\n",
      "Train-epoch 17. Iteration 00100, Avg-Loss: 0.1120, Accuracy: 0.9900\n",
      "Train-epoch 17. Iteration 00200, Avg-Loss: 0.1344, Accuracy: 0.9800\n",
      "Train-epoch 17. Iteration 00300, Avg-Loss: 0.1482, Accuracy: 0.9600\n",
      "Train-epoch 17. Iteration 00400, Avg-Loss: 0.1499, Accuracy: 0.9600\n",
      "Train-epoch 17. Iteration 00500, Avg-Loss: 0.1715, Accuracy: 0.9520\n",
      "Validation-epoch 17. Iteration 00134, Avg-Loss: 5.7081, Accuracy: 0.2256\n",
      "Train-epoch 18. Iteration 00100, Avg-Loss: 0.3661, Accuracy: 0.8600\n",
      "Train-epoch 18. Iteration 00200, Avg-Loss: 0.2617, Accuracy: 0.9000\n",
      "Train-epoch 18. Iteration 00300, Avg-Loss: 0.2680, Accuracy: 0.8933\n",
      "Train-epoch 18. Iteration 00400, Avg-Loss: 0.2330, Accuracy: 0.9125\n",
      "Train-epoch 18. Iteration 00500, Avg-Loss: 0.2330, Accuracy: 0.9100\n",
      "Validation-epoch 18. Iteration 00134, Avg-Loss: 6.6046, Accuracy: 0.3910\n",
      "Train-epoch 19. Iteration 00100, Avg-Loss: 0.2724, Accuracy: 0.8700\n",
      "Train-epoch 19. Iteration 00200, Avg-Loss: 0.2212, Accuracy: 0.9050\n",
      "Train-epoch 19. Iteration 00300, Avg-Loss: 0.2041, Accuracy: 0.9133\n",
      "Train-epoch 19. Iteration 00400, Avg-Loss: 0.1819, Accuracy: 0.9275\n",
      "Train-epoch 19. Iteration 00500, Avg-Loss: 0.1757, Accuracy: 0.9300\n",
      "Validation-epoch 19. Iteration 00134, Avg-Loss: 6.7148, Accuracy: 0.3835\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "#my_model = models.resnet18(pretrained=True)\n",
    "#my_model.fc = nn.Linear(my_model.fc.in_features,categories)\n",
    "my_model = AverageModel(output_size=categories)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "epochs = 20\n",
    "\n",
    "train_model(my_model, loss_fn, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
