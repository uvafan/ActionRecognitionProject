{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "res_model = models.resnet50(pretrained=True)\n",
    "print(res_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g01_c01', 'g01_c02', 'g01_c03', 'g01_c04', 'g01_c05', 'g01_c06', 'g02_c01', 'g02_c02', 'g02_c03', 'g02_c04', 'g03_c01', 'g03_c02', 'g03_c03', 'g03_c04', 'g03_c05', 'g03_c06', 'g04_c01', 'g04_c02', 'g04_c03', 'g04_c04', 'g04_c05', 'g04_c06', 'g04_c07', 'g05_c01', 'g05_c02', 'g05_c03', 'g05_c04', 'g05_c05', 'g05_c06', 'g05_c07', 'g06_c01', 'g06_c02', 'g06_c03', 'g06_c04', 'g06_c05', 'g06_c06', 'g06_c07', 'g07_c01', 'g07_c02', 'g07_c03', 'g07_c04', 'g07_c05', 'g07_c06', 'g07_c07', 'g08_c01', 'g08_c02', 'g08_c03', 'g08_c04', 'g08_c05', 'g09_c01', 'g09_c02', 'g09_c03', 'g09_c04', 'g09_c05', 'g09_c06', 'g09_c07', 'g10_c01', 'g10_c02', 'g10_c03', 'g10_c04', 'g10_c05', 'g11_c01', 'g11_c02', 'g11_c03', 'g11_c04', 'g11_c05', 'g12_c01', 'g12_c02', 'g12_c03', 'g12_c04', 'g12_c05', 'g12_c06', 'g13_c01', 'g13_c02', 'g13_c03', 'g13_c04', 'g13_c05', 'g13_c06', 'g14_c01', 'g14_c02', 'g14_c03', 'g14_c04', 'g14_c05', 'g15_c01', 'g15_c02', 'g15_c03', 'g15_c04', 'g15_c05', 'g15_c06', 'g15_c07', 'g16_c01', 'g16_c02', 'g16_c03', 'g16_c04', 'g16_c05', 'g17_c01', 'g17_c02', 'g17_c03', 'g17_c04', 'g17_c05', 'g18_c01', 'g18_c02', 'g18_c03', 'g18_c04', 'g18_c05', 'g19_c01', 'g19_c02', 'g19_c03', 'g19_c04', 'g20_c01', 'g20_c02', 'g20_c03', 'g20_c04', 'g20_c05', 'g20_c06', 'g21_c01', 'g21_c02', 'g21_c03', 'g21_c04', 'g21_c05', 'g22_c01', 'g22_c02', 'g22_c03', 'g22_c04', 'g22_c05', 'g23_c01', 'g23_c02', 'g23_c03', 'g23_c04', 'g23_c05', 'g23_c06', 'g24_c01', 'g24_c02', 'g24_c03', 'g24_c04', 'g24_c05', 'g24_c06', 'g24_c07', 'g25_c01', 'g25_c02', 'g25_c03', 'g25_c04', 'g25_c05', 'g25_c06', 'g25_c07']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('data/data_first_25/ApplyEyeMakeup'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0339, -0.2803, -0.5588, -1.0633, -0.5982, -0.1185, -0.7245,  0.4723,\n",
      "          0.1687, -0.7725, -0.8772, -0.9897, -0.3863, -0.9537, -0.8534, -0.4497,\n",
      "         -0.6492, -0.1997, -0.1648, -0.4842, -1.0847, -0.7866, -1.3034, -0.1153,\n",
      "         -1.0191, -0.7022, -0.8275, -0.6509, -0.5022, -0.4131, -0.7665, -0.8926,\n",
      "         -0.6493, -0.4952, -0.2028, -0.5817,  0.5857, -0.6630, -0.3757,  0.1591,\n",
      "         -0.5846, -0.6555, -0.6937, -0.2647, -0.2696, -0.2190, -0.5557, -0.4581,\n",
      "         -1.1228, -0.7563, -0.2625,  0.3987, -0.2595, -0.3165, -0.1794, -0.9064,\n",
      "         -0.4023, -0.9893, -0.2564,  0.0165,  0.7054,  0.2354, -0.0092,  0.0259,\n",
      "         -0.5421, -0.1671, -0.3316, -0.1988, -0.6164, -0.4905, -1.3046,  0.3017,\n",
      "         -1.3353, -0.5006, -0.8859, -0.8446,  0.1442, -0.3220,  0.4130,  0.0800,\n",
      "         -0.8237, -1.5672,  0.1075, -0.7383, -0.5086, -0.1458,  0.0601,  0.4678,\n",
      "         -0.0493, -0.4882, -0.6295, -0.9816, -1.3854, -0.4129,  0.0985, -1.4554,\n",
      "         -0.3752, -0.6398, -1.1034, -0.3851, -0.8985, -0.6859, -0.7673, -0.3249,\n",
      "         -0.2561, -0.9004, -0.6850, -1.1488, -0.7714, -1.1816, -1.3059, -0.6691,\n",
      "          0.9172,  0.3499,  0.5403, -1.4633, -0.7400, -0.3105,  0.5581, -0.1205,\n",
      "         -0.7305, -0.1907,  0.2468,  0.0809,  0.9194, -0.1681,  0.3339, -1.2033,\n",
      "         -0.9396, -1.1074, -1.0576, -1.1627, -1.0956, -1.4365, -0.4511, -1.0183,\n",
      "         -0.7324, -1.2114, -1.4655, -1.3438, -1.1321, -1.3861, -1.8985, -1.2623,\n",
      "         -0.7469, -0.1798, -0.9186, -1.3940, -1.0991, -1.2244,  0.1613,  1.3019,\n",
      "         -0.6288, -0.3731,  0.0415,  0.5078, -0.1503,  0.1216,  0.0897,  0.2687,\n",
      "          0.2336,  0.4029,  0.2937,  0.7217,  0.1861, -0.2087, -0.2603, -0.4403,\n",
      "          0.4799, -0.1617,  0.0047,  0.7641,  0.2870,  0.2458,  0.1146, -0.4568,\n",
      "          0.0343, -0.1178,  0.4809,  0.4670,  0.3795,  0.1754,  0.5300,  0.2935,\n",
      "          0.6781,  0.6715,  0.6389,  0.4158,  0.2479,  0.6554, -0.2729,  0.3841,\n",
      "          0.4861,  0.6587, -0.5822,  0.8511,  0.1703,  0.2676,  0.0726,  0.6383,\n",
      "          0.1639, -0.0004,  0.2409,  0.5026,  0.1180,  0.1444, -0.0885,  0.4162,\n",
      "          1.1102,  0.5241, -0.2149,  0.5232,  0.1902,  0.0016,  0.2106,  0.2255,\n",
      "          0.1280,  0.1557, -0.1441,  0.3052,  0.3565, -0.0306,  0.0897,  0.5953,\n",
      "          0.3038,  0.5103,  0.2024,  0.7872, -0.4044, -0.1646,  0.0445,  0.5570,\n",
      "          0.2711, -0.2303,  0.6368,  0.6347,  0.6406,  0.3872,  0.5985,  0.1028,\n",
      "          0.3560,  0.0162,  0.5313,  0.3215, -0.1825,  0.3009,  0.2648,  0.0538,\n",
      "          0.4179,  0.0562,  0.5588,  0.8501, -0.4849,  0.6048,  0.8856, -0.5435,\n",
      "          0.5619,  0.2061,  0.1791,  0.1888, -0.1065, -0.2494, -0.1059,  0.2807,\n",
      "          0.5539,  0.6662,  0.3010,  0.4282,  0.1170, -0.6203, -0.6980, -0.7352,\n",
      "         -0.4023,  0.6582, -0.9708, -1.0374, -0.7851, -0.8297, -1.1679, -0.7514,\n",
      "         -0.1809,  0.8438,  0.5942,  0.1489,  0.4369,  0.9852, -0.5133, -0.4122,\n",
      "         -0.6643, -1.4580, -0.9544, -1.1666, -0.1527, -0.8878, -1.0689, -0.8468,\n",
      "         -0.9476, -1.2590, -0.5095, -0.2343, -1.2579, -0.8823, -0.2861, -0.3155,\n",
      "         -1.0827, -0.7941,  0.4006, -0.7442, -1.3354, -0.3283,  0.4408, -0.2144,\n",
      "         -0.2740,  0.3010,  0.5975, -0.5041, -1.0471, -1.2931, -0.9981, -0.4582,\n",
      "         -1.1895, -0.8077, -0.9990, -1.3612, -1.1669, -1.3539, -1.0057,  0.2488,\n",
      "         -0.2194, -0.5853,  0.0561,  0.0419,  0.0432,  0.0375, -0.3930, -0.6554,\n",
      "         -1.2620, -0.0292,  0.3298, -1.0070, -0.2688,  0.5153, -0.3754, -0.8725,\n",
      "         -1.0361,  0.2805, -0.6769, -1.3489, -0.1930, -1.1157, -0.9266, -1.9220,\n",
      "         -1.1524, -0.6532, -0.6760,  0.2611,  1.0484,  0.1191,  0.6028,  0.3199,\n",
      "         -0.2227,  0.2021, -0.1606,  0.0133, -0.3868, -0.2869, -1.2828, -0.4002,\n",
      "         -0.8874, -0.8042, -0.5445, -0.3291, -0.7333,  0.0726, -0.4559, -0.8306,\n",
      "         -1.0902,  0.0852, -0.2098, -0.5067,  0.1622, -0.5386, -0.2321, -0.7579,\n",
      "         -0.8296, -0.5123, -0.8290, -1.1669, -1.1162, -0.3021,  0.0871,  0.1422,\n",
      "         -1.2181, -1.5783, -0.3790,  0.0721, -0.8461, -0.8392,  0.1826,  0.3826,\n",
      "         -0.3868,  0.5766,  0.1005, -1.7681, -1.8077, -0.6956, -0.2870, -0.6100,\n",
      "         -0.4820,  1.2181, -0.5801,  0.1505,  1.3898,  0.5082,  0.4304,  0.8407,\n",
      "         -0.3014,  0.5702,  0.0501,  1.0037,  0.6578,  0.7382, -0.0175,  0.0172,\n",
      "         -0.0469, -1.1144,  0.3770,  1.1499,  1.1717,  0.3157, -0.2589, -0.3569,\n",
      "          0.2878,  0.5853,  0.6513,  0.9729, -0.1702, -0.4334,  0.5432,  0.2279,\n",
      "          1.0053,  0.1314,  0.0231, -0.3397, -0.4281,  0.3636,  0.0290,  1.3017,\n",
      "          0.6853, -0.9012, -0.5149,  0.3244,  0.5544, -0.1072, -0.1483,  0.5492,\n",
      "          1.1110,  1.1753, -0.1199,  0.7578, -0.4776,  0.6926,  1.0162,  2.0084,\n",
      "          0.6901, -0.2587, -1.2484,  0.0816, -0.2876,  1.3146,  0.7926,  0.3730,\n",
      "         -0.1550,  0.4679, -0.2966,  0.1521,  0.0720,  0.3576,  0.6867,  0.4290,\n",
      "          0.1966,  0.0381,  0.0679, -0.8577, -1.3699,  0.0078, -0.3182,  1.0806,\n",
      "          1.3975,  0.6506,  0.3527,  0.6322,  0.2752, -0.9431,  0.9131, -0.9030,\n",
      "         -0.0787, -0.3534, -0.2409,  0.9511, -1.5947,  0.6083,  1.0034,  0.5848,\n",
      "          0.9324,  0.7562,  0.8832,  0.3028,  0.3453,  0.0887, -0.9683, -0.5169,\n",
      "          0.7159,  0.3617,  0.8365,  1.5944,  0.0063, -0.1533,  1.0127,  0.7464,\n",
      "         -0.9459,  0.4325,  0.5545,  1.4628,  0.2432, -1.0037, -0.1533, -0.4951,\n",
      "          0.4571,  0.2400,  0.9198,  0.3952,  0.0766, -0.3941,  0.3293, -0.6031,\n",
      "         -0.4247, -0.7791, -0.3357,  1.0659, -0.8895,  1.3984,  0.8205,  0.3964,\n",
      "          0.3023,  0.9239,  0.3173, -1.8355, -0.9052, -0.2055, -0.2883,  0.1890,\n",
      "          0.9121, -0.1039, -1.1934, -0.6441,  0.2456,  0.5024,  1.0347,  0.7757,\n",
      "          0.0848, -0.0648,  0.6181, -0.2019, -1.1641, -0.6441,  0.3462,  0.9721,\n",
      "          0.3524, -0.6069,  0.8994, -0.0326,  0.9424, -1.2355,  0.8729, -0.1273,\n",
      "         -0.8453,  1.0456,  0.5638,  0.1522,  0.1660, -0.2878,  0.7445,  0.6754,\n",
      "          0.4388,  0.7552, -0.3019,  1.3227,  0.4925,  0.9578, -0.2787,  0.4990,\n",
      "         -0.7973,  0.7635,  0.4042, -0.7565,  1.1628, -0.1939, -0.4338,  0.6470,\n",
      "          2.0600,  0.2300, -0.2189, -0.4528,  0.4397,  0.4325,  1.0136, -0.2192,\n",
      "          0.7408, -0.4241,  0.8154,  0.4348, -0.3640,  0.3278, -0.0363,  0.2418,\n",
      "          0.7966,  0.6615,  1.5865,  0.7318,  0.9959,  0.5810,  0.2997,  0.4340,\n",
      "         -0.0668, -0.8810,  0.8124, -0.1747, -1.0952, -0.0573, -0.0141,  0.7269,\n",
      "          0.6783,  1.0163, -0.1289,  0.5450,  0.9107,  0.6281,  0.9463,  0.3358,\n",
      "         -1.5324,  1.0600,  0.1876,  1.2353,  0.5731, -0.8043,  0.4928,  0.2325,\n",
      "         -0.8183, -1.1964,  1.0698,  0.0422,  0.7667,  0.7128, -0.1282,  0.8140,\n",
      "          0.0521, -0.0519,  0.0028,  0.3767, -0.2294, -0.9154, -0.2578, -1.1149,\n",
      "          0.5766, -0.0219,  1.2263,  0.6574, -0.7504, -0.7224,  0.1244, -0.0027,\n",
      "         -0.2511,  0.6244,  0.9846, -0.8000,  1.4166,  0.8551,  0.9852,  0.1659,\n",
      "          0.6710,  0.6538, -0.4133,  0.4987,  0.6108, -0.9423, -0.3014, -0.8987,\n",
      "         -0.1526, -0.7366, -0.6491,  0.9121,  0.9799,  0.4322, -1.0073,  0.7811,\n",
      "          1.3690,  0.0854, -0.4588,  0.5281,  1.6994,  0.0590, -0.2583,  0.1921,\n",
      "          0.4222, -0.0909, -0.4551,  0.6974,  0.6993,  0.1320,  0.3875,  0.7065,\n",
      "          0.0411, -0.2973,  0.3542, -0.4229,  0.5038, -0.7436, -0.2537,  0.5872,\n",
      "          0.4382,  0.1968,  1.2150,  0.1216, -0.5727,  0.9867, -0.7604, -0.1610,\n",
      "          1.4070, -0.4993,  0.0169,  1.8699, -0.4314,  1.6064, -1.0934,  0.0476,\n",
      "         -0.1299,  1.0532,  0.8409,  0.1174,  0.7917,  0.1058,  0.3280,  0.3173,\n",
      "          0.3411, -0.1631,  0.0983,  0.4174,  0.5084,  1.0312,  0.3502, -0.3407,\n",
      "          0.2768,  0.6024,  0.8239, -0.5200,  0.7126, -0.0023,  1.2089, -0.0255,\n",
      "          0.1473,  0.7308,  0.6201,  0.2858,  1.0930,  0.6451,  0.4407,  0.3971,\n",
      "         -0.1434,  1.1375,  0.5170,  0.1356,  1.1474,  0.4922,  0.7249,  0.5305,\n",
      "          0.4147,  0.2108,  1.2101, -0.6958, -0.8483, -0.7774,  0.7965,  0.7450,\n",
      "          1.3933,  0.2810,  0.6054,  0.8336,  0.4538,  0.1702,  0.5131,  0.7590,\n",
      "          1.2332,  1.0643,  0.1471,  0.2260,  0.8637,  0.6718, -0.6090,  0.4500,\n",
      "         -0.3427,  0.2472, -1.1176, -1.2998,  1.0139,  0.8303,  0.3295,  0.1903,\n",
      "          1.3934,  0.1552, -0.3210,  0.9127, -0.3648,  1.4198, -0.9287, -0.2785,\n",
      "          0.3912, -0.9906,  1.1781,  0.1617, -1.4179, -1.1366,  0.2621,  0.8023,\n",
      "          0.9141, -0.7941,  0.7273,  0.7684,  1.2293, -0.5574,  1.1081,  0.1106,\n",
      "         -0.7654, -0.9010,  0.5370,  0.2103,  1.5390,  1.6241,  1.0755, -0.6747,\n",
      "          1.2401,  0.5300,  0.4959,  0.2378, -0.0350,  1.4246,  0.5320, -0.4287,\n",
      "          0.1120,  0.7258,  1.0687,  1.3856,  1.7532, -0.3680, -0.0833,  0.8630,\n",
      "         -0.8925, -0.3367, -0.1283,  0.6665,  0.4484,  1.1453,  0.5494, -0.1781,\n",
      "         -0.5684,  0.5149,  0.1131, -0.4744,  1.0276, -0.1649,  0.6650, -1.2616,\n",
      "          0.8331, -0.8130, -1.9059,  0.2063,  1.2060, -0.1760, -0.1430,  1.3753,\n",
      "          0.9578, -0.0534,  0.8983,  0.9689,  0.0273,  0.3283, -0.0945, -0.3033,\n",
      "         -0.9601,  0.5323, -0.7210,  0.1592,  0.7661, -0.1544, -0.3699, -0.9321,\n",
      "          0.7871,  0.6378,  1.5903,  1.5595, -0.9143, -0.0679,  1.4354,  0.9179,\n",
      "          0.6926,  0.0751, -0.0568,  1.1216, -0.8923,  0.4930,  1.4623,  1.3364,\n",
      "          0.7595, -0.6943, -1.8413, -0.3958,  0.4038,  0.3109,  0.6325,  0.1140,\n",
      "         -0.2120,  0.9008, -0.6726,  0.4339,  0.0159, -0.8301, -0.8479, -0.5306,\n",
      "         -0.1553,  1.2679,  0.0083,  0.1316,  0.3101, -1.3664,  0.1959, -0.3392,\n",
      "          0.2606,  0.5912,  0.2892,  0.2253, -0.3706, -0.1937,  0.2915,  0.3620,\n",
      "         -0.0216, -0.2745, -0.8341,  0.3103,  0.6349,  0.1484,  0.1753, -0.2212,\n",
      "         -0.0498,  0.1388,  0.8545, -0.3470, -0.0640, -0.1297, -0.1690, -0.6635,\n",
      "          0.1572,  0.1648, -0.2281, -0.4759, -0.7720, -0.1714,  0.6526, -0.1995,\n",
      "          0.9123,  0.0967, -0.1028,  1.0707, -0.2433, -0.3648, -1.4959,  0.5408,\n",
      "         -1.2321,  0.5076,  0.1842, -1.0706, -0.6885, -0.0004,  0.4256, -0.2178,\n",
      "         -0.5906, -0.7093, -1.8422,  1.3575, -0.0261, -0.7111, -0.3027, -0.7792,\n",
      "         -0.7742, -1.7864, -0.5015, -0.0309,  0.4081, -0.4251,  1.0680,  0.8762]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#import imageio\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.Resize(256),  # 1. Resize smallest side to 256.\n",
    "     transforms.CenterCrop(224), # 2. Crop center square of 224x224 pixels.\n",
    "     transforms.ToTensor(), # 3. Convert to pytorch tensor.\n",
    "     transforms.Normalize(mean = [0.485, 0.456, 0.406],  # normalize.\n",
    "                          std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "img_pil = Image.open('data/data_first_25/ApplyEyeMakeup/g01_c01/v_ApplyEyeMakeup_g01_c01_frame0.jpg')\n",
    "\n",
    "# 1. Forward propagate the image through the CNN.\n",
    "# Unsqueeze adds a dummy batch dimension needed to pass through the model.\n",
    "input_img =  test_transform(img_pil).unsqueeze(0)\n",
    "\n",
    "print(res_model(input_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_img_list(cat,d1):\n",
    "    ret = list()\n",
    "    for frame in os.listdir('data/data_first_25/{}/{}'.format(cat,d1)):\n",
    "        img_pil = Image.open('data/data_first_25/{}/{}/{}'.format(cat,d1,frame))\n",
    "        input_img = test_transform(img_pil).unsqueeze(0)\n",
    "        ret.append(input_img)\n",
    "    return ret\n",
    "\n",
    "def createTrainAndValSet(categories,trainPercentage):\n",
    "    category_options = os.listdir('data/data_first_25')\n",
    "    category_names = category_options[:categories]\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    i=0\n",
    "    for cat in category_names:\n",
    "        for d1 in os.listdir('data/data_first_25/{}'.format(cat)):\n",
    "            r = random.uniform(0,1)\n",
    "            img_list = get_img_list(cat,d1)\n",
    "            if r < trainPercentage:\n",
    "                train_set.append((img_list,i))\n",
    "            else:\n",
    "                val_set.append((img_list,i))\n",
    "        i+=1\n",
    "    return train_set,val_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/HHTseng/video-classification/blob/master/ResNetCRNN/functions.py\n",
    "\n",
    "\n",
    "\n",
    "# 2D CNN encoder using ResNet-152 pretrained\n",
    "class ResCNNEncoder(nn.Module):\n",
    "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(ResCNNEncoder, self).__init__()\n",
    "\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
    "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # ResNet CNN\n",
    "            with torch.no_grad():\n",
    "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
    "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
    "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,        \n",
    "            num_layers=h_RNN_layers,       \n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "        \n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_used = 5\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "batchSize = 1\n",
    "learningRate = 1e-2\n",
    "epochs = 1\n",
    "trainPercentage = 1\n",
    "\n",
    "train_set, val_set = createTrainAndValSet(categories_used,trainPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00010, Avg-Loss: 3.9009, Accuracy: 0.3000\n",
      "Train-epoch 0. Iteration 00020, Avg-Loss: 3.1230, Accuracy: 0.2000\n",
      "Train-epoch 0. Iteration 00030, Avg-Loss: 2.4519, Accuracy: 0.3667\n",
      "Train-epoch 0. Iteration 00040, Avg-Loss: 2.9166, Accuracy: 0.3250\n",
      "Train-epoch 0. Iteration 00050, Avg-Loss: 2.7968, Accuracy: 0.2800\n",
      "Train-epoch 0. Iteration 00060, Avg-Loss: 2.6507, Accuracy: 0.2667\n",
      "Train-epoch 0. Iteration 00070, Avg-Loss: 2.4757, Accuracy: 0.3143\n",
      "Train-epoch 0. Iteration 00080, Avg-Loss: 2.4715, Accuracy: 0.2875\n",
      "Train-epoch 0. Iteration 00090, Avg-Loss: 2.3948, Accuracy: 0.2667\n",
      "Train-epoch 0. Iteration 00100, Avg-Loss: 2.3231, Accuracy: 0.2800\n",
      "Train-epoch 0. Iteration 00110, Avg-Loss: 2.2718, Accuracy: 0.2727\n",
      "Train-epoch 0. Iteration 00120, Avg-Loss: 2.2261, Accuracy: 0.2667\n",
      "Train-epoch 0. Iteration 00130, Avg-Loss: 2.1910, Accuracy: 0.2538\n",
      "Train-epoch 0. Iteration 00140, Avg-Loss: 2.1565, Accuracy: 0.2500\n",
      "Train-epoch 0. Iteration 00150, Avg-Loss: 2.1296, Accuracy: 0.2400\n",
      "Train-epoch 0. Iteration 00160, Avg-Loss: 2.1036, Accuracy: 0.2313\n",
      "Train-epoch 0. Iteration 00170, Avg-Loss: 2.0772, Accuracy: 0.2353\n",
      "Train-epoch 0. Iteration 00180, Avg-Loss: 2.0624, Accuracy: 0.2389\n",
      "Train-epoch 0. Iteration 00190, Avg-Loss: 2.0441, Accuracy: 0.2316\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 2.0268, Accuracy: 0.2250\n",
      "Train-epoch 0. Iteration 00210, Avg-Loss: 1.9921, Accuracy: 0.2381\n",
      "Train-epoch 0. Iteration 00220, Avg-Loss: 2.0109, Accuracy: 0.2273\n",
      "Train-epoch 0. Iteration 00230, Avg-Loss: 1.9998, Accuracy: 0.2304\n",
      "Train-epoch 0. Iteration 00240, Avg-Loss: 1.9876, Accuracy: 0.2250\n",
      "Train-epoch 0. Iteration 00250, Avg-Loss: 1.9779, Accuracy: 0.2160\n",
      "Train-epoch 0. Iteration 00260, Avg-Loss: 1.9613, Accuracy: 0.2192\n",
      "Train-epoch 0. Iteration 00270, Avg-Loss: 1.9409, Accuracy: 0.2296\n",
      "Train-epoch 0. Iteration 00280, Avg-Loss: 1.9389, Accuracy: 0.2286\n",
      "Train-epoch 0. Iteration 00290, Avg-Loss: 1.9326, Accuracy: 0.2207\n",
      "Train-epoch 0. Iteration 00300, Avg-Loss: 1.9214, Accuracy: 0.2167\n",
      "Train-epoch 0. Iteration 00310, Avg-Loss: 1.9179, Accuracy: 0.2194\n",
      "Train-epoch 0. Iteration 00320, Avg-Loss: 1.9084, Accuracy: 0.2156\n",
      "Train-epoch 0. Iteration 00330, Avg-Loss: 1.9037, Accuracy: 0.2152\n",
      "Train-epoch 0. Iteration 00340, Avg-Loss: 1.8931, Accuracy: 0.2206\n",
      "Train-epoch 0. Iteration 00350, Avg-Loss: 1.8808, Accuracy: 0.2143\n",
      "Train-epoch 0. Iteration 00360, Avg-Loss: 1.8775, Accuracy: 0.2139\n",
      "Train-epoch 0. Iteration 00370, Avg-Loss: 1.8746, Accuracy: 0.2108\n",
      "Train-epoch 0. Iteration 00380, Avg-Loss: 1.8701, Accuracy: 0.2158\n",
      "Train-epoch 0. Iteration 00390, Avg-Loss: 1.8670, Accuracy: 0.2154\n",
      "Train-epoch 0. Iteration 00400, Avg-Loss: 1.8608, Accuracy: 0.2150\n",
      "Train-epoch 0. Iteration 00410, Avg-Loss: 1.8568, Accuracy: 0.2146\n",
      "Train-epoch 0. Iteration 00420, Avg-Loss: 1.8498, Accuracy: 0.2143\n",
      "Train-epoch 0. Iteration 00430, Avg-Loss: 1.8467, Accuracy: 0.2116\n",
      "Train-epoch 0. Iteration 00440, Avg-Loss: 1.8426, Accuracy: 0.2091\n",
      "Train-epoch 0. Iteration 00450, Avg-Loss: 1.8399, Accuracy: 0.2067\n",
      "Train-epoch 0. Iteration 00460, Avg-Loss: 1.8363, Accuracy: 0.2043\n",
      "Train-epoch 0. Iteration 00470, Avg-Loss: 1.8338, Accuracy: 0.2043\n",
      "Train-epoch 0. Iteration 00480, Avg-Loss: 1.8293, Accuracy: 0.2062\n",
      "Train-epoch 0. Iteration 00490, Avg-Loss: 1.8277, Accuracy: 0.2061\n",
      "Train-epoch 0. Iteration 00500, Avg-Loss: 1.8256, Accuracy: 0.2060\n",
      "Train-epoch 0. Iteration 00510, Avg-Loss: 1.8216, Accuracy: 0.2078\n",
      "Train-epoch 0. Iteration 00520, Avg-Loss: 1.8171, Accuracy: 0.2096\n",
      "Train-epoch 0. Iteration 00530, Avg-Loss: 1.8132, Accuracy: 0.2094\n",
      "Train-epoch 0. Iteration 00540, Avg-Loss: 1.8131, Accuracy: 0.2074\n",
      "Train-epoch 0. Iteration 00550, Avg-Loss: 1.8103, Accuracy: 0.2073\n",
      "Train-epoch 0. Iteration 00560, Avg-Loss: 1.8072, Accuracy: 0.2089\n",
      "Train-epoch 0. Iteration 00570, Avg-Loss: 1.8048, Accuracy: 0.2105\n",
      "Train-epoch 0. Iteration 00580, Avg-Loss: 1.8030, Accuracy: 0.2103\n",
      "Train-epoch 0. Iteration 00590, Avg-Loss: 1.7999, Accuracy: 0.2102\n",
      "Train-epoch 0. Iteration 00600, Avg-Loss: 1.7976, Accuracy: 0.2083\n",
      "Train-epoch 0. Iteration 00610, Avg-Loss: 1.7936, Accuracy: 0.2049\n",
      "Train-epoch 0. Iteration 00620, Avg-Loss: 1.7912, Accuracy: 0.2081\n",
      "Train-epoch 0. Iteration 00630, Avg-Loss: 1.7910, Accuracy: 0.2063\n",
      "Train-epoch 0. Iteration 00640, Avg-Loss: 1.7883, Accuracy: 0.2094\n",
      "Train-epoch 1. Iteration 00010, Avg-Loss: 1.3920, Accuracy: 0.3000\n",
      "Train-epoch 1. Iteration 00020, Avg-Loss: 1.5658, Accuracy: 0.1500\n",
      "Train-epoch 1. Iteration 00030, Avg-Loss: 1.6656, Accuracy: 0.1667\n",
      "Train-epoch 1. Iteration 00040, Avg-Loss: 1.6514, Accuracy: 0.1750\n",
      "Train-epoch 1. Iteration 00050, Avg-Loss: 1.6515, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00060, Avg-Loss: 1.7087, Accuracy: 0.1833\n",
      "Train-epoch 1. Iteration 00070, Avg-Loss: 1.6962, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00080, Avg-Loss: 1.6902, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00090, Avg-Loss: 1.6756, Accuracy: 0.1889\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 1.6549, Accuracy: 0.2100\n",
      "Train-epoch 1. Iteration 00110, Avg-Loss: 1.6412, Accuracy: 0.2091\n",
      "Train-epoch 1. Iteration 00120, Avg-Loss: 1.6178, Accuracy: 0.2417\n",
      "Train-epoch 1. Iteration 00130, Avg-Loss: 1.6501, Accuracy: 0.2308\n",
      "Train-epoch 1. Iteration 00140, Avg-Loss: 1.6569, Accuracy: 0.2286\n",
      "Train-epoch 1. Iteration 00150, Avg-Loss: 1.6593, Accuracy: 0.2133\n",
      "Train-epoch 1. Iteration 00160, Avg-Loss: 1.6582, Accuracy: 0.2125\n",
      "Train-epoch 1. Iteration 00170, Avg-Loss: 1.6592, Accuracy: 0.2118\n",
      "Train-epoch 1. Iteration 00180, Avg-Loss: 1.6588, Accuracy: 0.2056\n",
      "Train-epoch 1. Iteration 00190, Avg-Loss: 1.6625, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 1.6599, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00210, Avg-Loss: 1.6585, Accuracy: 0.2000\n",
      "Train-epoch 1. Iteration 00220, Avg-Loss: 1.6608, Accuracy: 0.1909\n",
      "Train-epoch 1. Iteration 00230, Avg-Loss: 1.6598, Accuracy: 0.1957\n",
      "Train-epoch 1. Iteration 00240, Avg-Loss: 1.6580, Accuracy: 0.2042\n",
      "Train-epoch 1. Iteration 00250, Avg-Loss: 1.6623, Accuracy: 0.1960\n",
      "Train-epoch 1. Iteration 00260, Avg-Loss: 1.6608, Accuracy: 0.1923\n",
      "Train-epoch 1. Iteration 00270, Avg-Loss: 1.6611, Accuracy: 0.1889\n",
      "Train-epoch 1. Iteration 00280, Avg-Loss: 1.6621, Accuracy: 0.1821\n",
      "Train-epoch 1. Iteration 00290, Avg-Loss: 1.6619, Accuracy: 0.1759\n",
      "Train-epoch 1. Iteration 00300, Avg-Loss: 1.6613, Accuracy: 0.1833\n",
      "Train-epoch 1. Iteration 00310, Avg-Loss: 1.6613, Accuracy: 0.1774\n",
      "Train-epoch 1. Iteration 00320, Avg-Loss: 1.6586, Accuracy: 0.1844\n",
      "Train-epoch 1. Iteration 00330, Avg-Loss: 1.6593, Accuracy: 0.1788\n",
      "Train-epoch 1. Iteration 00340, Avg-Loss: 1.6565, Accuracy: 0.1765\n",
      "Train-epoch 1. Iteration 00350, Avg-Loss: 1.6662, Accuracy: 0.1743\n",
      "Train-epoch 1. Iteration 00360, Avg-Loss: 1.6653, Accuracy: 0.1778\n",
      "Train-epoch 1. Iteration 00370, Avg-Loss: 1.6662, Accuracy: 0.1784\n",
      "Train-epoch 1. Iteration 00380, Avg-Loss: 1.6639, Accuracy: 0.1842\n",
      "Train-epoch 1. Iteration 00390, Avg-Loss: 1.6635, Accuracy: 0.1846\n",
      "Train-epoch 1. Iteration 00400, Avg-Loss: 1.6636, Accuracy: 0.1850\n",
      "Train-epoch 1. Iteration 00410, Avg-Loss: 1.6633, Accuracy: 0.1902\n",
      "Train-epoch 1. Iteration 00420, Avg-Loss: 1.6647, Accuracy: 0.1881\n",
      "Train-epoch 1. Iteration 00430, Avg-Loss: 1.6664, Accuracy: 0.1907\n",
      "Train-epoch 1. Iteration 00440, Avg-Loss: 1.6662, Accuracy: 0.1886\n",
      "Train-epoch 1. Iteration 00450, Avg-Loss: 1.6634, Accuracy: 0.1933\n",
      "Train-epoch 1. Iteration 00460, Avg-Loss: 1.6634, Accuracy: 0.1957\n",
      "Train-epoch 1. Iteration 00470, Avg-Loss: 1.6674, Accuracy: 0.1915\n",
      "Train-epoch 1. Iteration 00480, Avg-Loss: 1.6671, Accuracy: 0.1938\n",
      "Train-epoch 1. Iteration 00490, Avg-Loss: 1.6662, Accuracy: 0.1918\n",
      "Train-epoch 1. Iteration 00500, Avg-Loss: 1.6656, Accuracy: 0.1920\n",
      "Train-epoch 1. Iteration 00510, Avg-Loss: 1.6664, Accuracy: 0.1902\n",
      "Train-epoch 1. Iteration 00520, Avg-Loss: 1.6657, Accuracy: 0.1885\n",
      "Train-epoch 1. Iteration 00530, Avg-Loss: 1.6653, Accuracy: 0.1906\n",
      "Train-epoch 1. Iteration 00540, Avg-Loss: 1.6640, Accuracy: 0.1944\n",
      "Train-epoch 1. Iteration 00550, Avg-Loss: 1.6633, Accuracy: 0.1945\n",
      "Train-epoch 1. Iteration 00560, Avg-Loss: 1.6616, Accuracy: 0.1964\n",
      "Train-epoch 1. Iteration 00570, Avg-Loss: 1.6648, Accuracy: 0.1930\n",
      "Train-epoch 1. Iteration 00580, Avg-Loss: 1.6641, Accuracy: 0.1931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 1. Iteration 00590, Avg-Loss: 1.6655, Accuracy: 0.1898\n",
      "Train-epoch 1. Iteration 00600, Avg-Loss: 1.6638, Accuracy: 0.1933\n",
      "Train-epoch 1. Iteration 00610, Avg-Loss: 1.6635, Accuracy: 0.1951\n",
      "Train-epoch 1. Iteration 00620, Avg-Loss: 1.6629, Accuracy: 0.1935\n",
      "Train-epoch 1. Iteration 00630, Avg-Loss: 1.6630, Accuracy: 0.1952\n",
      "Train-epoch 1. Iteration 00640, Avg-Loss: 1.6633, Accuracy: 0.1938\n",
      "Train-epoch 2. Iteration 00010, Avg-Loss: 1.6127, Accuracy: 0.0000\n",
      "Train-epoch 2. Iteration 00020, Avg-Loss: 1.6508, Accuracy: 0.1500\n",
      "Train-epoch 2. Iteration 00030, Avg-Loss: 1.6601, Accuracy: 0.1667\n",
      "Train-epoch 2. Iteration 00040, Avg-Loss: 1.6795, Accuracy: 0.1750\n",
      "Train-epoch 2. Iteration 00050, Avg-Loss: 1.6403, Accuracy: 0.1600\n",
      "Train-epoch 2. Iteration 00060, Avg-Loss: 1.6524, Accuracy: 0.1500\n",
      "Train-epoch 2. Iteration 00070, Avg-Loss: 1.6308, Accuracy: 0.1857\n",
      "Train-epoch 2. Iteration 00080, Avg-Loss: 1.6453, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00090, Avg-Loss: 1.6487, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 1.6397, Accuracy: 0.2100\n",
      "Train-epoch 2. Iteration 00110, Avg-Loss: 1.6384, Accuracy: 0.2182\n",
      "Train-epoch 2. Iteration 00120, Avg-Loss: 1.6448, Accuracy: 0.2333\n",
      "Train-epoch 2. Iteration 00130, Avg-Loss: 1.6500, Accuracy: 0.2231\n",
      "Train-epoch 2. Iteration 00140, Avg-Loss: 1.6533, Accuracy: 0.2214\n",
      "Train-epoch 2. Iteration 00150, Avg-Loss: 1.6506, Accuracy: 0.2267\n",
      "Train-epoch 2. Iteration 00160, Avg-Loss: 1.6590, Accuracy: 0.2188\n",
      "Train-epoch 2. Iteration 00170, Avg-Loss: 1.6613, Accuracy: 0.2118\n",
      "Train-epoch 2. Iteration 00180, Avg-Loss: 1.6605, Accuracy: 0.2111\n",
      "Train-epoch 2. Iteration 00190, Avg-Loss: 1.6593, Accuracy: 0.2105\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 1.6537, Accuracy: 0.2100\n",
      "Train-epoch 2. Iteration 00210, Avg-Loss: 1.6825, Accuracy: 0.2190\n",
      "Train-epoch 2. Iteration 00220, Avg-Loss: 1.6809, Accuracy: 0.2273\n",
      "Train-epoch 2. Iteration 00230, Avg-Loss: 1.6812, Accuracy: 0.2174\n",
      "Train-epoch 2. Iteration 00240, Avg-Loss: 1.6838, Accuracy: 0.2083\n",
      "Train-epoch 2. Iteration 00250, Avg-Loss: 1.6813, Accuracy: 0.2040\n",
      "Train-epoch 2. Iteration 00260, Avg-Loss: 1.6793, Accuracy: 0.2115\n",
      "Train-epoch 2. Iteration 00270, Avg-Loss: 1.6805, Accuracy: 0.2111\n",
      "Train-epoch 2. Iteration 00280, Avg-Loss: 1.6771, Accuracy: 0.2143\n",
      "Train-epoch 2. Iteration 00290, Avg-Loss: 1.6780, Accuracy: 0.2138\n",
      "Train-epoch 2. Iteration 00300, Avg-Loss: 1.6771, Accuracy: 0.2067\n",
      "Train-epoch 2. Iteration 00310, Avg-Loss: 1.6787, Accuracy: 0.2032\n",
      "Train-epoch 2. Iteration 00320, Avg-Loss: 1.6748, Accuracy: 0.2094\n",
      "Train-epoch 2. Iteration 00330, Avg-Loss: 1.6724, Accuracy: 0.2061\n",
      "Train-epoch 2. Iteration 00340, Avg-Loss: 1.6677, Accuracy: 0.2118\n",
      "Train-epoch 2. Iteration 00350, Avg-Loss: 1.6646, Accuracy: 0.2086\n",
      "Train-epoch 2. Iteration 00360, Avg-Loss: 1.6656, Accuracy: 0.2111\n",
      "Train-epoch 2. Iteration 00370, Avg-Loss: 1.6628, Accuracy: 0.2135\n",
      "Train-epoch 2. Iteration 00380, Avg-Loss: 1.6664, Accuracy: 0.2105\n",
      "Train-epoch 2. Iteration 00390, Avg-Loss: 1.6649, Accuracy: 0.2103\n",
      "Train-epoch 2. Iteration 00400, Avg-Loss: 1.6624, Accuracy: 0.2075\n",
      "Train-epoch 2. Iteration 00410, Avg-Loss: 1.6620, Accuracy: 0.2098\n",
      "Train-epoch 2. Iteration 00420, Avg-Loss: 1.6601, Accuracy: 0.2048\n",
      "Train-epoch 2. Iteration 00430, Avg-Loss: 1.6585, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00440, Avg-Loss: 1.6552, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00450, Avg-Loss: 1.6583, Accuracy: 0.1956\n",
      "Train-epoch 2. Iteration 00460, Avg-Loss: 1.6566, Accuracy: 0.2022\n",
      "Train-epoch 2. Iteration 00470, Avg-Loss: 1.6559, Accuracy: 0.2064\n",
      "Train-epoch 2. Iteration 00480, Avg-Loss: 1.6588, Accuracy: 0.2042\n",
      "Train-epoch 2. Iteration 00490, Avg-Loss: 1.6597, Accuracy: 0.2020\n",
      "Train-epoch 2. Iteration 00500, Avg-Loss: 1.6599, Accuracy: 0.2020\n",
      "Train-epoch 2. Iteration 00510, Avg-Loss: 1.6597, Accuracy: 0.2020\n",
      "Train-epoch 2. Iteration 00520, Avg-Loss: 1.6601, Accuracy: 0.2019\n",
      "Train-epoch 2. Iteration 00530, Avg-Loss: 1.6599, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00540, Avg-Loss: 1.6615, Accuracy: 0.1981\n",
      "Train-epoch 2. Iteration 00550, Avg-Loss: 1.6607, Accuracy: 0.1964\n",
      "Train-epoch 2. Iteration 00560, Avg-Loss: 1.6592, Accuracy: 0.1982\n",
      "Train-epoch 2. Iteration 00570, Avg-Loss: 1.6577, Accuracy: 0.2018\n",
      "Train-epoch 2. Iteration 00580, Avg-Loss: 1.6592, Accuracy: 0.2000\n",
      "Train-epoch 2. Iteration 00590, Avg-Loss: 1.6582, Accuracy: 0.2017\n",
      "Train-epoch 2. Iteration 00600, Avg-Loss: 1.6584, Accuracy: 0.2017\n",
      "Train-epoch 2. Iteration 00610, Avg-Loss: 1.6558, Accuracy: 0.2049\n",
      "Train-epoch 2. Iteration 00620, Avg-Loss: 1.6565, Accuracy: 0.2048\n",
      "Train-epoch 2. Iteration 00630, Avg-Loss: 1.6569, Accuracy: 0.2048\n",
      "Train-epoch 2. Iteration 00640, Avg-Loss: 1.6574, Accuracy: 0.2047\n",
      "Train-epoch 3. Iteration 00010, Avg-Loss: 1.6169, Accuracy: 0.3000\n",
      "Train-epoch 3. Iteration 00020, Avg-Loss: 1.6238, Accuracy: 0.3000\n",
      "Train-epoch 3. Iteration 00030, Avg-Loss: 1.6133, Accuracy: 0.3000\n",
      "Train-epoch 3. Iteration 00040, Avg-Loss: 1.6236, Accuracy: 0.2750\n",
      "Train-epoch 3. Iteration 00050, Avg-Loss: 1.6314, Accuracy: 0.2600\n",
      "Train-epoch 3. Iteration 00060, Avg-Loss: 1.6363, Accuracy: 0.2333\n",
      "Train-epoch 3. Iteration 00070, Avg-Loss: 1.6318, Accuracy: 0.2286\n",
      "Train-epoch 3. Iteration 00080, Avg-Loss: 1.6350, Accuracy: 0.2125\n",
      "Train-epoch 3. Iteration 00090, Avg-Loss: 1.6317, Accuracy: 0.2111\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 1.6288, Accuracy: 0.2100\n",
      "Train-epoch 3. Iteration 00110, Avg-Loss: 1.6374, Accuracy: 0.2091\n",
      "Train-epoch 3. Iteration 00120, Avg-Loss: 1.6431, Accuracy: 0.2000\n",
      "Train-epoch 3. Iteration 00130, Avg-Loss: 1.6486, Accuracy: 0.1923\n",
      "Train-epoch 3. Iteration 00140, Avg-Loss: 1.6446, Accuracy: 0.2000\n",
      "Train-epoch 3. Iteration 00150, Avg-Loss: 1.6439, Accuracy: 0.2000\n",
      "Train-epoch 3. Iteration 00160, Avg-Loss: 1.6474, Accuracy: 0.1938\n",
      "Train-epoch 3. Iteration 00170, Avg-Loss: 1.6509, Accuracy: 0.1824\n",
      "Train-epoch 3. Iteration 00180, Avg-Loss: 1.6503, Accuracy: 0.1833\n",
      "Train-epoch 3. Iteration 00190, Avg-Loss: 1.6483, Accuracy: 0.1842\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 1.6487, Accuracy: 0.1950\n",
      "Train-epoch 3. Iteration 00210, Avg-Loss: 1.6473, Accuracy: 0.1952\n",
      "Train-epoch 3. Iteration 00220, Avg-Loss: 1.6497, Accuracy: 0.1909\n",
      "Train-epoch 3. Iteration 00230, Avg-Loss: 1.6477, Accuracy: 0.1870\n",
      "Train-epoch 3. Iteration 00240, Avg-Loss: 1.6479, Accuracy: 0.1833\n",
      "Train-epoch 3. Iteration 00250, Avg-Loss: 1.6480, Accuracy: 0.1800\n",
      "Train-epoch 3. Iteration 00260, Avg-Loss: 1.6469, Accuracy: 0.1808\n",
      "Train-epoch 3. Iteration 00270, Avg-Loss: 1.6490, Accuracy: 0.1815\n",
      "Train-epoch 3. Iteration 00280, Avg-Loss: 1.6484, Accuracy: 0.1786\n",
      "Train-epoch 3. Iteration 00290, Avg-Loss: 1.6494, Accuracy: 0.1759\n",
      "Train-epoch 3. Iteration 00300, Avg-Loss: 1.6456, Accuracy: 0.1800\n",
      "Train-epoch 3. Iteration 00310, Avg-Loss: 1.6439, Accuracy: 0.1774\n",
      "Train-epoch 3. Iteration 00320, Avg-Loss: 1.6420, Accuracy: 0.1781\n",
      "Train-epoch 3. Iteration 00330, Avg-Loss: 1.6362, Accuracy: 0.1818\n",
      "Train-epoch 3. Iteration 00340, Avg-Loss: 1.6333, Accuracy: 0.1794\n",
      "Train-epoch 3. Iteration 00350, Avg-Loss: 1.6323, Accuracy: 0.1800\n",
      "Train-epoch 3. Iteration 00360, Avg-Loss: 1.6398, Accuracy: 0.1806\n",
      "Train-epoch 3. Iteration 00370, Avg-Loss: 1.6397, Accuracy: 0.1811\n",
      "Train-epoch 3. Iteration 00380, Avg-Loss: 1.6404, Accuracy: 0.1789\n",
      "Train-epoch 3. Iteration 00390, Avg-Loss: 1.6441, Accuracy: 0.1769\n",
      "Train-epoch 3. Iteration 00400, Avg-Loss: 1.6444, Accuracy: 0.1775\n",
      "Train-epoch 3. Iteration 00410, Avg-Loss: 1.6458, Accuracy: 0.1756\n",
      "Train-epoch 3. Iteration 00420, Avg-Loss: 1.6451, Accuracy: 0.1762\n",
      "Train-epoch 3. Iteration 00430, Avg-Loss: 1.6441, Accuracy: 0.1791\n",
      "Train-epoch 3. Iteration 00440, Avg-Loss: 1.6420, Accuracy: 0.1841\n",
      "Train-epoch 3. Iteration 00450, Avg-Loss: 1.6391, Accuracy: 0.1844\n",
      "Train-epoch 3. Iteration 00460, Avg-Loss: 1.6354, Accuracy: 0.1848\n",
      "Train-epoch 3. Iteration 00470, Avg-Loss: 1.6351, Accuracy: 0.1851\n",
      "Train-epoch 3. Iteration 00480, Avg-Loss: 1.6389, Accuracy: 0.1833\n",
      "Train-epoch 3. Iteration 00490, Avg-Loss: 1.6392, Accuracy: 0.1837\n",
      "Train-epoch 3. Iteration 00500, Avg-Loss: 1.6393, Accuracy: 0.1860\n",
      "Train-epoch 3. Iteration 00510, Avg-Loss: 1.6396, Accuracy: 0.1863\n",
      "Train-epoch 3. Iteration 00520, Avg-Loss: 1.6382, Accuracy: 0.1885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 3. Iteration 00530, Avg-Loss: 1.6375, Accuracy: 0.1887\n",
      "Train-epoch 3. Iteration 00540, Avg-Loss: 1.6371, Accuracy: 0.1889\n",
      "Train-epoch 3. Iteration 00550, Avg-Loss: 1.6368, Accuracy: 0.1873\n",
      "Train-epoch 3. Iteration 00560, Avg-Loss: 1.6374, Accuracy: 0.1857\n",
      "Train-epoch 3. Iteration 00570, Avg-Loss: 1.6367, Accuracy: 0.1860\n",
      "Train-epoch 3. Iteration 00580, Avg-Loss: 1.6358, Accuracy: 0.1897\n",
      "Train-epoch 3. Iteration 00590, Avg-Loss: 1.6351, Accuracy: 0.1898\n",
      "Train-epoch 3. Iteration 00600, Avg-Loss: 1.6367, Accuracy: 0.1883\n",
      "Train-epoch 3. Iteration 00610, Avg-Loss: 1.6374, Accuracy: 0.1869\n",
      "Train-epoch 3. Iteration 00620, Avg-Loss: 1.6371, Accuracy: 0.1855\n",
      "Train-epoch 3. Iteration 00630, Avg-Loss: 1.6373, Accuracy: 0.1841\n",
      "Train-epoch 3. Iteration 00640, Avg-Loss: 1.6372, Accuracy: 0.1844\n",
      "Train-epoch 4. Iteration 00010, Avg-Loss: 1.6389, Accuracy: 0.2000\n",
      "Train-epoch 4. Iteration 00020, Avg-Loss: 1.6380, Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "learningRate = 5e-3\n",
    "epochs = 10\n",
    "\n",
    "train_accuracies = []; val_accuracies = []\n",
    "train_losses = []; val_losses = []\n",
    "\n",
    "\n",
    "def train_model(categories,lossFn,batchSize,learningRate,epochs,trainPercentage):\n",
    "#     model = models.resnet50(pretrained=True).cuda()\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    cnn_encoder = ResCNNEncoder().cuda()\n",
    "    rnn_decoder = DecoderRNN().cuda()\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "\n",
    "    lossFn = lossFn.cuda()\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     for param in model.fc.parameters():\n",
    "#         param.requires_grad = True\n",
    "#     model.fc = nn.Linear(in_features=2048,out_features=categories).cuda()\n",
    "#     model.fc.requires_grad = True\n",
    "#     optimizer = optim.Adam(model.fc.parameters(),lr=learningRate,weight_decay=1e-3)\n",
    "\n",
    "    crnn_params = list(cnn_encoder.fc1.parameters()) + list(cnn_encoder.bn1.parameters()) + \\\n",
    "                  list(cnn_encoder.fc2.parameters()) + list(cnn_encoder.bn2.parameters()) + \\\n",
    "                  list(cnn_encoder.fc3.parameters()) + list(rnn_decoder.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(crnn_params, lr=learningRate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        shuffle(train_set)\n",
    "        correct = 0\n",
    "        cum_loss = 0\n",
    "        \n",
    "#         model.train()\n",
    "        i=0\n",
    "        for video in train_set:\n",
    "            frame_list,target_cat = video\n",
    "            frame_list = torch.stack(frame_list, dim=0).transpose(0, 1).cuda()\n",
    "#             print(frame_list.size())\n",
    "            scores = torch.tensor(np.zeros((1,categories)),dtype=torch.float).cuda()\n",
    "#             for frame in frame_list:\n",
    "#                 frame = frame.cuda()\n",
    "#                 frame_scores = model(frame)\n",
    "#                 scores += frame_scores\n",
    "#             scores/=len(frame_list)\n",
    "\n",
    "\n",
    "#             scores = model(frame_list[0].cuda())\n",
    "            scores = rnn_decoder(cnn_encoder(frame_list))\n",
    "    \n",
    "            loss = lossFn(scores,torch.tensor(np.array([target_cat]),dtype=torch.long).cuda())\n",
    "#             print(scores)\n",
    "#             print(target_cat)\n",
    "#             print(loss)\n",
    "            max_score, max_label = scores.max(1)\n",
    "            if max_label == target_cat:\n",
    "                correct+=1\n",
    "            cum_loss+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "                    (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
    "            i+=1\n",
    "        train_accuracies.append(correct / len(train_set))\n",
    "        train_losses.append(cum_loss / (i + 1))   \n",
    "\n",
    "   \n",
    "train_model(categories_used,lossFn,batchSize,learningRate,epochs,trainPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
